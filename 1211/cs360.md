---
title: CS 360 - Introduction to Theory of Computing
layout: toc
parent: Winter 2021
prof: John Watrous
---
This set of notes is basically  from the [Watrous text](https://cs.uwaterloo.ca/~watrous/ToC-notes/ToC-notes.pdf) with some addons and comments from the instructor or myself. This term, we are following [Version of June 17, 2017](https://student.cs.uwaterloo.ca/~cs360/cs360notes.pdf).

Also note that this set of notes is incomplete, inaccurate with some statements. It serves quick reference to Wastrous' text.

# Lec 1
Important idea:
> Computational problems, devices, and processes can themselves be viewed as
mathematical objects.

If you want to avoid this sort of paradox (Russell's), you need to replace naïve set theory
with axiomatic set theory, which is quite a bit more formal and disallows objects
such as the set of all sets (which is what opens the door to let in Russell’s paradox). For more interesting discussion, check [PMATH 433](/pmath433).

## Sets and countability

The **size** of a finite set is the number of elements if contains. If <span>&#92;(A &#92;)</span> is a finite set, then we write <span>&#92;(&#124; A &#124; &#92;)</span> to denote this number. Sets can also be infinite. Note that in this course, we will include 0 as a natural number.

Definition 1.1
: A set <span>&#92;(A &#92;)</span> is countable if either (i) <span>&#92;(A &#92;)</span> is empty, or (ii) there exists an
onto (or surjective) function of the form <span>&#92;(f:\mathbb N \to A &#92;)</span>. If a set is not countable, then
we say that it is uncountable.

These three statements are equivalent for any choice of a set <span>&#92;(A &#92;)</span>:
1. <span>&#92;(A &#92;)</span> is countable.
2. There exists a one-to-one (or injective) function of the form <span>&#92;(g:A\to \mathbb N &#92;)</span>.
3. Either <span>&#92;(A &#92;)</span> is finite or there exists a one-to-one and onto (or bijective) function of
the form <span>&#92;(h:\mathbb N\to A &#92;)</span>.

Definition 1.5
: For any set <span>&#92;(A &#92;)</span>, the power set of <span>&#92;(A &#92;)</span> is the set <span>&#92;(\mathcal P(A) &#92;)</span> containing all subsets of <span>&#92;(A &#92;)</span>: <span>&#92;(\mathcal P(A) &#92;left&#92;{ B:B\subseteq A &#92;right&#92;}&#92;)</span>.

Theorem 1.6
: (Cantor). The power set of the natural numbers is uncountable.

Proof omitted (page 10).  The method used in the proof above is called diagonalization, for reasons we will
discuss later in the course. This is a fundamentally important proof technique in
the theory of computation.

## Alphabets, strings, and languages
Definition 1.7
: An alphabet is a finite and nonempty set.

Typical names for alphabets in this course are capital Greek letters. We typically refer to elements of alphabets as symbols, and we will often
use lower-case Greek letters.

Definition 1.8
: Let <span>&#92;(\Sigma &#92;)</span> be an alphabet. A string over alphabet <span>&#92;(\Sigma &#92;)</span> is a finite, ordered sequence of symbols from <span>&#92;(\Sigma &#92;)</span>. The length of a string is the total number of symbols in the sequence.

There is a special string, called the empty string and denoted <span>&#92;(\varepsilon &#92;)</span>, that has no symbols in it (and therefore it has length 0). It is a string over every alphabet.

Definition 1.9
: Let <span>&#92;(\Sigma &#92;)</span> be an alphabet. A language over <span>&#92;(\Sigma &#92;)</span>  is a set of strings, with each string being a string over the alphabet <span>&#92;(\Sigma &#92;)</span>.

A simple but nevertheless important example of a language over a given alphabet <span>&#92;(\Sigma &#92;)</span> is the set of *all* strings over <span>&#92;(\Sigma &#92;)</span>. We denote this language as <span>&#92;(\Sigma^* &#92;)</span>.

In this course, lower-case Roman letters at the end of the alphabet (u, v, w, x, y, z): strings. Capital Roman letters near the beginning of
the alphabet (A, B, C, D): languages.

# Lec 2
##  Countability and languages

Proposition 2.1
: For every alphabet <span>&#92;(\Sigma &#92;)</span>, the language <span>&#92;(\Sigma^* &#92;)</span> is countable.

In this course, lexicographic order means strings are ordered first by length,
and by “dictionary” ordering among strings of the same length.

TFAE, for any choice of an alphabet <span>&#92;(\Sigma &#92;)</span>:
1. <span>&#92;(A &#92;)</span> is a language over the alphabet <span>&#92;(\Sigma &#92;)</span>.
2. <span>&#92;(A\subseteq \Sigma^* &#92;)</span>.
3. <span>&#92;(A\in \mathcal P(\Sigma^*) &#92;)</span>.

Proposition 2.3
: Let <span>&#92;(\Sigma &#92;)</span> be an alphabet. The set <span>&#92;(P(\Sigma^*) &#92;)</span> is uncountable.

## Deterministic finite automata
Definition 2.4
: A deterministic finite automaton (or DFA, for short) is a 5-tuple <span>&#92;(M=(Q,\Sigma, \delta, q _ 0, F), &#92;)</span> where <span>&#92;(Q &#92;)</span> is a finite  and nonempty set (whose elements we will call states), <span>&#92;(\Sigma &#92;)</span> is an alphabet, <span>&#92;(\delta &#92;)</span> is a function (called the transition function) having the form <span>&#92;(\delta: Q\times \Sigma \to Q, &#92;)</span> <span>&#92;(q _ 0\in Q &#92;)</span> is a state (called the start state), and <span>&#92;(F\subseteq Q &#92;)</span> is a subset of states (whose
elements we  will call accept states).

See page 17 for an example of DFA.

Note from cs 241, [Carmen's slide](https://cs.uwaterloo.ca/~cbruni/CS241Resources/lectures/2019_Winter/CS241L06_formal_languages_and_dfas_post.pdf), in this course (cs 360),
> Some people allow a transition diagram to omit some of the arrows out of a state.
Implicitly, such transitions cause the automaton to reject. For this course, however,
DO NOT omit any arrows for a DFA. There is no implicit state for “always reject”;
if you want one specify it explicitly. (For the case of an NFA, see later.)

from the instructor's comment.


### DFA computations
We already know what it means for a DFA to accept or reject a given input string <span>&#92;(w\in \Sigma^* &#92;)</span>.

Definition 2.5
: Let <span>&#92;(M=(Q,\Sigma, \delta, q _ 0, F) &#92;)</span> be a DFA and let <span>&#92;(w\in \Sigma^* &#92;)</span> be a string. The DFA M accepts the string <span>&#92;(w &#92;)</span> if one of the following statements holds:
1. <span>&#92;(w = \varepsilon &#92;)</span> and <span>&#92;(q _ 0\in F &#92;)</span>.
2. <span>&#92;(w=\sigma _ 1\cdots \sigma _ n  &#92;)</span> for a positive integer <span>&#92;(n &#92;)</span> and symbols <span>&#92;(\sigma _ 1,\ldots, \sigma _ n \in \Sigma &#92;)</span>, and there exist states <span>&#92;(r _ 0, \ldots, r _ n \in Q &#92;)</span> such that <span>&#92;(r _ 0 = q _ 0, r _ n \in F &#92;)</span>, and <span>&#92;(r _ {k+1} = \delta (r _ k, \delta _ {k+1}) &#92;)</span> for all <span>&#92;(k \in &#92;left&#92;{ 0, \ldots, n-1 &#92;right&#92;} &#92;)</span>.

If <span>&#92;(M &#92;)</span> does not accept <span>&#92;(w &#92;)</span>, then <span>&#92;(M &#92;)</span> rejects <span>&#92;(w &#92;)</span>.

It's sometimes useful to define a new function <span>&#92;(\delta ^ * : Q\times \Sigma^ * \to Q &#92;)</span>. Intuitively speaking, <span>&#92;(\delta^* (q, w) &#92;)</span> is the state you end up on if you start at state <span>&#92;(q &#92;)</span> and follow the transitions specified by the string <span>&#92;(w &#92;)</span>.

### Languages recognized by DFAs and regular languages
Suppose <span>&#92;(M &#92;)</span> is a DFA. We may then consider the set of all strings
that are accepted by M. This language is denoted
<span>&#92;[
    {\rm L}(M) = &#92;left&#92;{ w\in \Sigma^* : M \text{ accepts }w &#92;right&#92;}.
&#92;]</span>
We e refer to this as the language recognized by <span>&#92;(M &#92;)</span>.

Definition 2.7
: Let <span>&#92;(\Sigma &#92;)</span> be an alphabet and let <span>&#92;(A\subseteq \Sigma^* &#92;)</span> be a language over <span>&#92;(\Sigma &#92;)</span>. The language <span>&#92;(A &#92;)</span> is regular if there exists a DFA <span>&#92;(M &#92;)</span> such that <span>&#92;(A={\rm L}(M) &#92;)</span>.

For a given alphabet <span>&#92;(\Sigma &#92;)</span>,
 the number of regular languages over the
alphabet <span>&#92;(\Sigma &#92;)</span> is countable.

Because there are uncountably many languages <span>&#92;(A\subseteq \Sigma^* &#92;)</span>, and only countably many regular languages <span>&#92;(A\subseteq \Sigma^* &#92;)</span>, we can immediately conclude that some languages are not regular.

# Lec 3
## Nondeterministic finite automata basics
Definition 3.1
: A nondeterministic finite automaton (or NFA, for short) is a 5-tuple <span>&#92;(N = (Q, \Sigma, \delta, q _ 0, F) &#92;)</span>, where Q finite & nonempty set of states, Σ is an alphabet, δ is a transition
function having the form <span>&#92;(\delta: Q\times (\Sigma \cup &#92;left&#92;{ \varepsilon &#92;right&#92;}) \to \mathcal P(Q) &#92;)</span>, q<sub>0</sub> start state, F is a subset of accept states.

Key diff between this and DFA is the transition function. For DFA, <span>&#92;(\delta(q, \sigma) &#92;)</span> was a state. For NFA, each <span>&#92;(\delta (q,\delta) &#92;)</span> is not a state, but rather a subset of states. This subset represents all of the
possible states that the NFA could move to when in state q and reading symbol σ. Even possible to have δ(q, σ) = ∅.

Also note we allow <span>&#92;(\varepsilon &#92;)</span>-transitions, where an NFA may move from one state to another without
reading a symbol from the input. See [Carmen's slides](https://cs.uwaterloo.ca/~cbruni/CS241Resources/lectures/2019_Winter/CS241L08_nfas_epsilon_post.pdf) on epsilon transitions.

Definition 3.2
: Let <span>&#92;(N &#92;)</span> be an NFA and <span>&#92;(w\in \Sigma^* &#92;)</span> be a string. The NFA A accepts w if <span>&#92;(\exists m\in \mathbb N &#92;)</span>, a sequence of states <span>&#92;(r _ 0,\ldots, r _ m &#92;)</span>, and a sequence of either symbols or empty strings <span>&#92;(\sigma _ 1,\ldots, \sigma _ m\in \Sigma \cup &#92;left&#92;{ \varepsilon &#92;right&#92;} &#92;)</span> such that the following statements all hold:
1. <span>&#92;(r _ 0 = q _ 0 &#92;)</span>
2. <span>&#92;(r _ m \in F &#92;)</span>
3. <span>&#92;(w = \sigma _ 1 \cdots \sigma _ m &#92;)</span>
4. <span>&#92;(r _ {k+1}\in \delta ( r _ k, \sigma _ {k+1}) &#92;)</span> for every k = 0, ..., m-1

If <span>&#92;(N &#92;)</span> does not accept <span>&#92;(w &#92;)</span>, then <span>&#92;(N &#92;)</span> rejects <span>&#92;(w &#92;)</span>.

Along similar lines to what we did for DFAs, we can define an extended version
of the transition function of an NFA. We define a new function <span>&#92;(\delta^* : Q \times \Sigma ^ * \to \mathcal P(Q) &#92;)</span> as follows.

First we define <span>&#92;(\varepsilon &#92;)</span>-closure of any set <span>&#92;(R\subseteq R &#92;)</span> as <span>&#92;(\varepsilon (R) &#92;)</span> = {q ∈ Q : q is reachable from some r ∈ R by following
zero or more ε-transitions }. We can interpret this alternative definition as saying that <span>&#92;(\varepsilon (R) &#92;)</span> is the smallest subset
of Q that contains R and is such that you can never get out of this set by following
an ε-transition.

Then we can recursively define δ<sup>*</sup> (details omitted).

Intuitively speaking, δ<sup>*</sup>(q, w) is the set of all states that you could potentially reach
by starting on the state q, reading w, and making as many ε-transitions along the
way as you like.

Also similar to DFAs, the notion L(N) denotes the language recognized by an NFA N:
<span>&#92;[
    {\rm L}(N)= &#92;left&#92;{ w\in \Sigma^* : N \text{ accepts }w &#92;right&#92;}.
&#92;]</span>

## Equivalence of NFAs and DFAs

Theorem 3.3
: Let <span>&#92;(\Sigma &#92;)</span> be an alphabet and let <span>&#92;(A \subseteq \Sigma ^ * &#92;)</span> be a language. The language <span>&#92;(A &#92;)</span> is regular (i.e., recognized by a DFA) iff <span>&#92;(A={\rm L}(N) &#92;)</span> for some NFA <span>&#92;(N &#92;)</span>.

Simple: regular <span>&#92;(\implies &#92;)</span> NFA... We will use the description of an NFA N to define an equivalent DFA M using
a simple idea: each state of M will keep track of a subset of states of N.

# Lec 4
In this lecture we will discuss the regular operations, as well as regular expressions
and their relationship to regular languages.
## Regular operations
Definition 4.1
: Let <span>&#92;(\Sigma &#92;)</span> be an alphabet and let <span>&#92;(A,B\subseteq \Sigma^* &#92;)</span> be languages. The regular operations are as follows:
1. Union: <span>&#92;(A\cup B = &#92;left&#92;{ w: w\in A\text{ or }w\in B &#92;right&#92;} &#92;)</span>
2. Concatenation: <span>&#92;(AB = &#92;left&#92;{ wx: w\in A\text{ and }x\in B &#92;right&#92;} &#92;)</span>
3. Kleene star (or just star): <span>&#92;(A^* = &#92;left&#92;{ \varepsilon &#92;right&#92;} \cup A \cup AA \cup AAA\cup \cdots &#92;)</span>

In words, <span>&#92;(A^* &#92;)</span>
is the language obtained by selecting any finite number of strings
from A and concatenating them together. (This includes the possibility to select
no strings at all from A, where we follow the convention that concatenating
together no strings at all gives the empty string.)

Theorem 4.2
: The regular languages are closed with respect to the regular operations: if <span>&#92;(A,B\subseteq \Sigma^* &#92;)</span>
are regular languages, then the languages A ∪ B, AB, and A<sup>∗</sup>
are also regular.

*Proof.* define such NFAs.

Note that we cannot easily conclude <span>&#92;(A^* &#92;)</span> is regular (for a regular language A) using
 the facts that the regular languages are closed under union
and concatenation. We can only conclude finite unions is regular, not for infinite unions.

## Other closure properties of regular languages
Definition 4.3
: Let <span>&#92;(A\subseteq \Sigma^* &#92;)</span> be a language over the alphabet <span>&#92;(\Sigma &#92;)</span>. The complement of <span>&#92;(A &#92;)</span>: <span>&#92;(\bar A=\Sigma^* \setminus A &#92;)</span>.

Proposition 4.4
: Let <span>&#92;( A &#92;)</span> be a regular language. <span>&#92;(\bar A &#92;)</span> is also regular.

Proposition 4.5
： Let <span>&#92;( A, B &#92;)</span> be regular languages. <span>&#92;(A\cap B &#92;)</span> is also regular.

*Proof*: <span>&#92;(A\cap B = \overline{\bar A \cup \bar B} &#92;)</span>

## Regular expressions
Definition 4.6
: alphabet <span>&#92;(\Sigma &#92;)</span>. R is a regular expression over the
alphabet Σ if any of these holds:
1. <span>&#92;(R= \varnothing &#92;)</span>.
2. <span>&#92;(R=\varepsilon &#92;)</span>.
3. <span>&#92;(R=\sigma &#92;)</span> for some choice of <span>&#92;(\sigma\in \Sigma &#92;)</span>.
4. <span>&#92;(R = (R _ 1\cup R _ 2) &#92;)</span>
5. <span>&#92;(R = (R _ 1  R _ 2) &#92;)</span>
6. <span>&#92;(R = (R _ 1^*) &#92;)</span>.

Definition 4.7
: Let R be a regular expression over the alphabet Σ. The language
recognized by R, which is denoted L(R), is defined as follows:
1. <span>&#92;(R=\varnothing \implies L(R)=\varnothing &#92;)</span>
2. <span>&#92;(R=\varepsilon \implies L(R)=&#92;left&#92;{ \varepsilon &#92;right&#92;} &#92;)</span>
3. <span>&#92;(R=\sigma \implies L(R)=&#92;left&#92;{ \sigma &#92;right&#92;} &#92;)</span>
4. <span>&#92;(R=(R _ 1 \cup R _ 2) \implies L(R)= L(R _ 1)\cup L(R _ 2) &#92;)</span>
5. <span>&#92;(R=(R _ 1  R _ 2) \implies L(R)= L(R _ 1) L(R _ 2) &#92;)</span>
6. <span>&#92;(R=(R _ 1 ^ * ) \implies L(R)= L(R _ 1) ^ * &#92;)</span>

Order of precedence for regular operations
1. star (highest precedence)
2. concatenation
3. union (lowest precedence).

Proposition 4.8
: Let Σ be an alphabet and let R be a regular expression over the alphabet Σ. It holds that the language L(R) is regular.

Theorem 4.9
: Let Σ be an alphabet and let A ⊆ Σ<sup>*</sup>
be a regular language. There exists a
regular expression over the alphabet Σ such that L(R) = A.

Theorem 4.9 (together with Prop. 4.8) is known as “Kleene’s Theorem”. It is not at all obvious; perhaps even unbelievable at first.

# Lec 5
## The pumping lemma (for regular languages)

Lemma 5.1
: (Pumping lemma for regular languages). Let <span>&#92;(\Sigma &#92;)</span> be an alphabet and let <span>&#92;(A\subseteq \Sigma^* &#92;)</span> be a regular language. There exists a positive integer <span>&#92;(n &#92;)</span> (called a pumping length of <span>&#92;(A &#92;)</span>) that possesses the following property. For every string <span>&#92;(w\in A &#92;)</span> with <span>&#92;(|w|\ge n &#92;)</span>, it is possible to write <span>&#92;(w=xyz &#92;)</span> for some choice of strings <span>&#92;(x,y,z\in \Sigma^* &#92;)</span> such that
1. <span>&#92;(y\ne \varepsilon &#92;)</span>,
2. <span>&#92;(|xy|\le n &#92;)</span>, and
3. <span>&#92;(xy^i z\in A &#92;)</span> for all <span>&#92;(i\in \mathbb N &#92;)</span>.

The pumping lemma is essentially a precise, technical way of expressing one
simple consequence of the following fact:
> If a DFA with n or fewer states reads n or more symbols from an input
string, at least one of its states must have been visited more than once.

## Using the pumping lemma to prove nonregularity

We can use the pumping lemma to prove that certain languages are *not* regular using the technique of
proof by contradiction. In particular, we take the following steps:
1. For A being the language we hope to prove is nonregular, we make the assumption that A is regular. Operating under the assumption that the language A is
regular, we are free to apply the pumping lemma to it.
2. Using the property that the pumping lemma establishes for A, we derive a
contradiction. Just about always the contradiction will be that we conclude that
some particular string is contained in A that we know is actually not contained
in A.
3. Having derived a contradiction, we conclude that it was our assumption that A
is regular that led to the contradiction, and so we conclude that A is nonregular.

Proposition 5.2
: Let <span>&#92;(\Sigma = &#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span> be the binary alphabet and define a language over <span>&#92;(\Sigma &#92;)</span> as follows: <span>&#92;(A= &#92;left&#92;{ 0^m1^m:m\in \mathbb N &#92;right&#92;} &#92;)</span> is not regular.

*Proof.* Follow the procedure above.

Proposition 5.3
: Let <span>&#92;(\Sigma=&#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span> be the binary alphabet and define a language over <span>&#92;(\Sigma &#92;)</span> as follows: <span>&#92;(B=&#92;left&#92;{ 0^m1^r:m,n\in \mathbb N, m>r &#92;right&#92;} &#92;)</span> is not regular.

*Proof.* Follow the procedure above.

Proposition 5.5
: Let <span>&#92;(\Sigma = &#92;left&#92;{ 0 &#92;right&#92;} &#92;)</span> and define a language over <span>&#92;(\Sigma &#92;)</span> as follows: <span>&#92;(C= &#92;{  0^m:m &#92;)</span> is a perfect square <span>&#92;(&#92;} &#92;)</span> is not regular.

*Proof.* Follow the procedure above.

Notation: <span>&#92;(w^R &#92;)</span> denotes the reverse of the string <span>&#92;(w &#92;)</span>.

Proposition 5.6
: Let <span>&#92;(\Sigma = &#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span> and define a language over <span>&#92;(\Sigma &#92;)</span> as follows: <span>&#92;( D= &#92;left&#92;{ w\in \Sigma^*: w = w^R &#92;right&#92;}  &#92;)</span> is not regular.

*Proof.* Follow the procedure above.

## Nonregularity from closure properties

Proposition 5.7
: Let <span>&#92;(\Sigma =&#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span> and define a language over <span>&#92;(\Sigma &#92;)</span> as follows: <span>&#92;(E=&#92;left&#92;{ w\in \Sigma^*: w \ne w^R &#92;right&#92;} &#92;)</span> is not regular.

*Proof.* Regular languages are closed under complementation.

Proposition 5.8
: Let <span>&#92;(\Sigma = &#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span> and define a language over <span>&#92;(\Sigma &#92;)</span> as follows: <span>&#92;(F= &#92;{ w\in \Sigma^*:  w &#92;)</span> has more 0s than 1s <span>&#92;(&#92;} &#92;)</span> is not regular.

*Proof.* Regular languages are closed under intersection: <span>&#92;(F\cap L(0 ^ * 1 ^ *)=B &#92;)</span>.

# Lec 6
## Other operations on languages
### Reverse
- <span>&#92;(w \in \Sigma^* \xrightarrow{reverse} w^R &#92;)</span> (formal definition omitted).
- <span>&#92;(A \subseteq \Sigma^* \xrightarrow{reverse} A^R = &#92;left&#92;{ w^R:w\in A &#92;right&#92;} &#92;)</span>

Proposition 6.1
: Let <span>&#92;(\Sigma &#92;)</span> be an alphabet and let <span>&#92;(A\subseteq \Sigma^* &#92;)</span> be a regular language. The language <span>&#92;(A^R &#92;)</span> is regular.

Two proofs are presented.

### Symmetric difference
<span>&#92;(A\triangle B = (A\setminus B) \cup (B\setminus A) &#92;)</span>

It's not hard to see A,B regular, then the symmetric difference  of these two languages is also regular.

### Prefix, suffix, and substring
- prefix of w: any string you can
obtain from w by removing zero or more symbols from the right-hand side of w;
- suffix of w: any string you can obtain by removing zero or more symbols from the
left-hand side of w;
- substring of w is any string you can obtain by removing
zero or more symbols from either or both the left-hand side and right-hand side of w.

Proposition 6.2
: <span>&#92;(A &#92;)</span> regular. The languages <span>&#92;(Prefix(A),Suffix(A), Substring(A) &#92;)</span> are regular.

*Proof.* Build DFA/NFA.

## Example problems concerning regular languages

**Problem 6.1**. Let <span>&#92;(\Sigma=&#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span> and let <span>&#92;(A\subseteq \Sigma^* &#92;)</span> be an arbitrarily chosen regular language. <span>&#92;(B= &#92;{ uv: uv\in \Sigma^* \text{ and }u\sigma v\in A  &#92;)</span> for some choice of <span>&#92;(\sigma \in \Sigma &#92;} &#92;)</span>
is regular. (Note that the language B can be described in intuitive terms as follows:
it is the language of all strings that can be obtained by choosing a nonempty string
w from A and deleting one symbol of w.)

**Solution 1**. Describe an NFA for B.

**Alternative Solution**. Define <span>&#92;(M_ {p,q} &#92;)</span> for each choice of <span>&#92;(p,q\in Q &#92;)</span>.

**Problem 6.2**. Let <span>&#92;(\Sigma= &#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span> and let <span>&#92;(A\subseteq \Sigma^* &#92;)</span> be an arbitrarily chosen regular language. <span>&#92;(C=&#92;left&#92;{ vu:u,v\in \Sigma^* \text{ and }uv\in A &#92;right&#92;} &#92;)</span> is regular.

**Problem 6.3**. Give examples:
1. A, B nonregular, A ∪ B is regular.
2. A, B nonregular, AB is regular.
3. A nonregular, A<sup>*</sup> is regular.

See page 65 of the notes. Note the choice of <span>&#92;(\Sigma = &#92;left&#92;{ 0 &#92;right&#92;} &#92;)</span>.

# Lec 7
## Definitions of CFGs and CFLs
Definition 7.1
: A context-free grammar (or CFG for short) is a 4-tuple <span>&#92;(G=(V,\Sigma,R,S) &#92;)</span>, where <span>&#92;(V &#92;)</span>: finite, nonempty (whose elements are called *variables*), <span>&#92;(\Sigma &#92;)</span>: alphabet (disjoint from <span>&#92;(V &#92;)</span>), <span>&#92;(R &#92;)</span> is a finite and nonempty set of rules, each of the form <span>&#92;(A\to w &#92;)</span>, where <span>&#92;(A\in V, w\in (V\cup \Sigma)^* &#92;)</span>, and <span>&#92;(S\in V &#92;)</span> is a variable called the start variable.

Every CFG <span>&#92;(G=(V,\Sigma, R, S) &#92;)</span> generates a language <span>&#92;(L(G)\subseteq \Sigma^* &#92;)</span>.

Definition 7.4
: Let <span>&#92;(G=(V,\Sigma,R,S) &#92;)</span> be a CFG. The *yields relation* defined by <span>&#92;(G &#92;)</span> is a relation defined for pairs of strings over the alphabet <span>&#92;(V\cup \Sigma &#92;)</span> as follows: <span>&#92;(uAv \Rightarrow_G uwv &#92;)</span> for every choice of strings <span>&#92;(u,v,w\in (V\cup \Sigma)^* &#92;)</span> and a variable <span>&#92;(A\in V &#92;)</span>, provided that the rule <span>&#92;(A\to w &#92;)</span> is included in <span>&#92;(R &#92;)</span>.

The interpretation of this relation is that <span>&#92;(x \Rightarrow_G y &#92;)</span> for <span>&#92;(x,y\in (V\cup \Sigma)^* &#92;)</span>, when it is
possible to replace one of the variables appearing in x according to one of the rules
of G in order to obtain y.

Definition 7.5
: Let G be a CFG. For any two strings <span>&#92;(x,y\in (V\cup \Sigma)^ * &#92;)</span> it holds that <span>&#92;(x \overset{ * }{\Rightarrow} _ G y &#92;)</span> if there exists a positive integer m and  strings <span>&#92;(z _ 1, \ldots, z _ m \in (V\cup \Sigma)^ * &#92;)</span>such that <span>&#92;(x = z _ 1, y = z _ m&#92;)</span>, and <span>&#92;(z _ k \Rightarrow_G z _ {k+1} &#92;)</span> for all <span>&#92;(k \in &#92;left&#92;{ 1 ,\ldots, m-1 &#92;right&#92;} &#92;)</span>.

This relation holds when it is possible to transform x into y performing zero or more substitutions according to the rules of G.

Definition 7.6
: <span>&#92;(L(G)= &#92;left&#92;{ x\in \Sigma^*: S \overset{ * }{\Rightarrow} _ G x &#92;right&#92;} &#92;)</span>.

The sequence <span>&#92;(z _ 1,\ldots, z _ m &#92;)</span> is said to be a derivation of x.

Definition 7.7
: Let <span>&#92;(\Sigma &#92;)</span> be an alphabet and <span>&#92;(A\subseteq \Sigma^* &#92;)</span> be a language.  The language A is context-free if there exists a CFG <span>&#92;(G &#92;)</span> such that <span>&#92;(L(G)=A &#92;)</span>.

## Basic examples
<span>&#92;(&#92;left&#92;{ 0 ^ n 1 ^ n: n\in \mathbb N &#92;right&#92;} &#92;)</span> is context-free.

<span>&#92;(PAL= &#92;left&#92;{ w\in \Sigma^*: w = w^R &#92;right&#92;} &#92;)</span> over the alphabet <span>&#92;(\Sigma = &#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span> is context-free. It is palindrome.

Shorthand notation: means "or"
<span>&#92;[
    S \to 0S0 &#124; 1S1 &#124; 0 &#124; 1 &#124; \varepsilon
&#92;]</span>

**Example 7.10**:  Σ = {0, 1}. <span>&#92;(A= w\in \Sigma^*: &#124; w &#124; _ 0 = &#124; w &#124; _ 1 &#92;)</span>. number of 1's = number of 0's. It is generated by this CFG:
<span>&#92;[
    S\to 0S1S &#124; 1S0S &#124; \varepsilon
&#92;]</span>

<span>&#92;(L(G)\subseteq A &#92;)</span> is trivial. Use strong induction on <span>&#92;(&#124; w &#124; &#92;)</span> to prove <span>&#92;(A\subseteq L(G) &#92;)</span>

**Example 7.12**. Consider <span>&#92;(\Sigma = &#92;left&#92;{ (, ) &#92;right&#92;} &#92;)</span>. Define a language <span>&#92;(BAL =&#92;{w\in \Sigma^*: w &#92;)</span> is properly balanced <span>&#92;(&#92;} &#92;)</span>. Context free: <span>&#92;(S\to (&#92;,S&#92;, )&#92;, S &#124; &#92;, \varepsilon &#92;)</span>

# Lec 8
## Left-most derivations
Consider CFG from the previous lecture:
<span>&#92;[
    S\to 0\, S\, 1\, S\, &#124;\, 1\, S\, 0\, S\, &#124;\, \varepsilon
&#92;]</span>
and
<span>&#92;[
    L(G) = &#92;left&#92;{ w\in \Sigma^*: &#124; w &#124; _ 0 = &#124; w &#124; _1 &#92;right&#92;}
&#92;]</span>

Here is an example of a derivation of string 0101:
```
S ⇒ 0 S 1 S ⇒ 0 1 S 0 S 1 S ⇒ 0 1 0 S 1 S ⇒ 0 1 0 1 S ⇒ 0 1 0 1
```
This is an example of a **left-most derivation**, which means that it is always the leftmost variable that gets replaced at each step.

Every string
that can be generated by a particular context-free grammar can also be generated
by that same grammar using a left-most derivation.

## Parse trees
With any derivation of a string by a context-free grammar we may associate a tree,
called a parse tree, according to the following rules:
- node: var/symbol/<span>&#92;(\varepsilon &#92;)</span>, root: start var.
- each node corresponding to a symbol or <span>&#92;(\varepsilon &#92;)</span> is a leaf node; each node corresponding to a var has one child  for each symbol
or variable with which it is replaced. The children of each (variable) node are
ordered in the same way as the symbols and variables in the rule used to replace
that variable.

The previous derivation yields this parse tree:

![](/mdf/pics/parsetree.png)

## Ambiguity
Sometimes a context-free grammar will allow multiple parse trees (or, equivalently,
multiple left-most derivations) for some strings in the language that it generates.

Here is a different derivation of string 0101:
```
S ⇒ 0 S 1 S ⇒ 0 1 S ⇒ 0 1 0 S 1 S ⇒ 0 1 0 1 S ⇒ 0 1 0 1.
```

corresponds to this parse tree:

![](/mdf/pics/parsetree2.png)

For a given CFG <span>&#92;(G &#92;)</span>, there exists at least one string <span>&#92;(w\in L(G) &#92;)</span> having at least two different parse trees, then the CFG <span>&#92;(G &#92;)</span> is said to be **ambiguous**.

### Designing unambiguous CFGs
For
example, we can come up with a different context-free grammar for the language (previous example: same number of 1s and 0s).
```
S → 0 X 1 S | 1 Y 0 S | ε
X → 0 X 1 X | ε
Y → 1 Y 0 Y | ε
```

Here is another example of how an ambiguous CFG can be modified to make it
unambiguous. Let us define an alphabet <span>&#92;(\Sigma = &#92;left&#92;{ a,b,+,*,(,) &#92;right&#92;} &#92;)</span> along with a CFG, ambiguous:
```
S → S + S | S ∗ S | ( S ) | a | b
```
We can, however, come up with a new CFG for the same language that is much
better—it is unambiguous and it properly captures the meaning of arithmetic expressions. Here it is:
```
S → T | S + T
T → F | T ∗ F
F → I | ( S )
I → a | b
```
Variables T generate *terms*, the variable F generates *factors*, and the variable I generates *identifiers*. An expression is either a
term or a sum of terms, a term is either a factor or a product of factors, and a factor
is either an identifier or an entire expression inside of parentheses.

## Inherently ambiguous languages
There are some context-free languages that can only be generated by ambiguous CFGs. Such languages are called **inherently ambiguous** context-free languages. An example of an inherently ambiguous context-free language is this one:
<span>&#92;[
    &#92;left&#92;{ 0^n1^m2^k:n=m \text{ or }m=k &#92;right&#92;}.
&#92;]</span>
Intuition: 0<sup>n</sup>1<sup>n</sup>2<sup>n</sup> will  always have multiple parse trees for some
sufficiently large natural number n.

##  Chomsky normal form
To be more precise about the specific sort of CFGs and parse trees we’re talking
about, it is appropriate at this point to define what is called the Chomsky normal
form for context-free grammars.

Definition 8.1
: A CFG <span>&#92;(G &#92;)</span> is in Chomsky normal form if every rule of <span>&#92;(G &#92;)</span> has one of the following three forms:
1. X → YZ, for variables X, Y, and Z, and where neither Y nor Z is the start
variable,
2. X → σ, for a variable X and a symbol σ, or
3. S → ε, for S the start variable.

Theorem 8.2
: Let Σ be an alphabet and let A ⊆ Σ<sup>*</sup>
be a context-free language. There
exists a CFG G in Chomsky normal form such that A = L(G).

The usual way to prove this theorem is through a construction that converts an
arbitrary CFG G into a CFG H in Chomsky normal form for which it holds that
L(H) = L(G).

Finally, it must be stressed that the Chomsky normal form says nothing about
ambiguity in general—a CFG in Chomsky normal form may or may not be ambiguous, just like we have for arbitrary CFGs. On the other hand, if you start with
an unambiguous CFG and perform the conversion described above, the resulting
CFG in Chomsky normal form will still be unambiguous

# Lec 9
##  Closure under the regular operations
Theorem 9.1
: A, B context-free. Then <span>&#92;(A\cup B, AB, A^* &#92;)</span> are context-free.

*Proof.* Construct <span>&#92;(G &#92;)</span>.

## Every regular language is context-free
Two ways to prove.

Theorem 9.2
: A regular language, then A is context-free.

*First proof*. Associate a CFG <span>&#92;(G &#92;)</span> by recursively applying simple constructions. Discuss by cases: 6 cases.

*Second proof*. DFA.

## Intersections of regular and context-free languages
Theorem 9.3
: A context-free, B regular. <span>&#92;(A\cap B &#92;)</span> context-free.

*Proof.* The main idea of the proof is to define a new CFG H such that L(H) = A ∩ B

## Prefixes, suffixes, and substrings
If A is context free, then Prefix(A), Suffix(A), Substring(A) are all context-free.

Construct CFGs for each.

# Lec 10
## Pumping lemma for CFL
If you have a parse tree for the derivation of a particular string by some context-free
grammar, and the parse tree is sufficiently deep, then there must be a variable that
appears multiple times on some path from the root to a leaf.

Lemma 10.1
: (Pumping lemma for context-free languages). <span>&#92;(A &#92;)</span> context-free. There exists a positive integer <span>&#92;(n &#92;)</span> (pumping length of <span>&#92;(A &#92;)</span>) that possesses the following property. For every string <span>&#92;(w\in A &#92;)</span> with <span>&#92;(&#124; w &#124;\ge n &#92;)</span>, it is possible to write <span>&#92;(w=uvxyz &#92;)</span> for some choise of strings <span>&#92;(u,v,x,y,z\in \Sigma^* &#92;)</span> such that
1. <span>&#92;(vy\ne \varepsilon &#92;)</span>
2. <span>&#92;(&#124; vxy &#124;\le n &#92;)</span>, and
3. <span>&#92;(u v^i x y^i z\in A &#92;)</span> for all <span>&#92;(i\in \mathbb N &#92;)</span>.

## Use this lemma

<span>&#92;(A= &#92;left&#92;{ 0^m 1^m 2^m: m\in \mathbb N &#92;right&#92;} &#92;)</span> is not context-free.

<span>&#92;(B = &#92;{ 0^m: m &#92;)</span> is a perfect square <span>&#92;(&#92;} &#92;)</span> is not context-free.

Every context-free
language over a single-symbol alphabet must be regular. (Proof skipped)

Σ = {0, 1, #}. <span>&#92;(C= &#92;{ r &#92;# s: r,s\in &#92;left&#92;{ 0,1 &#92;right&#92;}^*,  &#92;)</span> r is a substring of s <span>&#92;(&#92;} &#92;)</span> is not context-free.

Discuss by cases: # in u, v, x, y, z.

## Closure properties
A, B CFL <span>&#92;(\not\implies A\cap B, \bar A &#92;)</span>  are context-free.

# Lec 11
## Pushdown automata
The **pushdown automaton** (or PDA) model of computation is essentially what you
get if you equip NFAs each with a single stack. A transition labeled by
- an input symbol or ε: read a symbol or take an ε-transition
- (↓, a): push symbol a onto the stack
- (↑, a): pop the symbol a off of the stack.

Definition 11.1
: PDF is 6-tuple <span>&#92;(P=(Q,\Sigma, \Gamma, \delta, q_0, F) &#92;)</span>. Q finite nonempty set of states. Σ is an alphabet (called the input alphabet). Γ is an alphabet (called the stack alphabet), δ is a function. F is a set of accept states.

It is required <span>&#92;(\Sigma \cap \operatorname{Stack}(\Gamma) = \varnothing &#92;)</span> where <span>&#92;(\operatorname{Stack}(\Gamma) = &#92;left&#92;{ \downarrow, \uparrow &#92;right&#92;}\times \Gamma &#92;)</span>.

Assume we read it from left to
right and imagine that we started with an empty stack. If a string does represent a
valid sequence of stack operations, we will say that it is a valid **stack string**; and if a
string fails to represent a valid sequence of stack operations, we will say that it is
an **invalid stack string**.

Definition 11.2
: Let P be a PDA, w be a string. P accepts w if there exists <span>&#92;(m\in \mathbb N &#92;)</span>, a sequence of states <span>&#92;(r _ 0,\ldots, r _ m &#92;)</span>, and a sequence <span>&#92;(a _ 1,\ldots, a _ m\in \Sigma \cup \operatorname{Stack}(\Gamma) \cup &#92;left&#92;{ \varepsilon &#92;right&#92;} &#92;)</span>. Then 4 properties to hold.

Shorthand notation: <span>&#92;(\sigma \uparrow a \downarrow b &#92;)</span> means σ is read, a is popped, b is pushed.

A CFL. B finite language. Then <span>&#92;(A\triangle B &#92;)</span> is context-free.

Let Σ and Γ be disjoint alphabets, let A ⊆ (Σ ∪ Γ)<sup>∗</sup>
be a context-free
language, and define <span>&#92;(B= &#92;{ w\in \Sigma^*: &#92;)</span>
there exists a string x ∈ A such that w is
obtained from x by deleting all symbols in Γ <span>&#92;(&#92;} &#92;)</span> is context-free.

## Equivalence of PDAs and CFGs
Every context-free language is recognized by a PDA. Every language recognized by a PDA is context-free

# Lec 12
**Church–Turing thesis**: Any function that can be computed by a mechanical process can be computed by a Turing machine

## Turing machine components and definitions
Three components of a TM:
1. The finite state control.
2. The tape head.
3. The tape.

Definition 12.1
: deterministic Turing machine (or DTM, for short) is a 7-tuple
<span>&#92;[
M=(Q, \Sigma, \Gamma, \delta, q _ 0, q _ {acc}, q _ {rej}),
&#92;]</span>
Q finite nonempty set of states; <span>&#92;(\Sigma &#92;)</span> input alphabet, may not include blank symbol <span>&#92;(\sqcup &#92;)</span>; <span>&#92;(\Gamma &#92;)</span> tape alphabet, must satisfy <span>&#92;(\Sigma \cup &#92;left&#92;{ \sqcup &#92;right&#92;} \subseteq \Gamma &#92;)</span>; <span>&#92;(\delta&#92;)</span> transition function; q0 initial state, q_acc, q_rej accept reject state.

## Turing machine computations
Three possible alternatives for a DTM M on a given input string w:
1. M accepts w.
2. M rejects w.
3. M runs forever on input w.

When we
speak of a configuration of a DTM, we are speaking of a description of all of the
Turing machine’s components at some instant:
1. the state of the finite state control,
2. the contents of the entire tape, and
3. the tape head position on the tape.

We express a config in the form
<span>&#92;[
u(q,a)v
&#92;]</span>
u has no blank on the left, v has no blank on the right. This config means: state of *M* is *q*; tape head of M is positioned over symbol *a* that occurs between *u* and *v*.

We define a function <span>&#92;(\gamma &#92;)</span>:
<span>&#92;[
\gamma(u(q,a)v) = \alpha (u)(q,a)\beta(v)
&#92;]</span>
just throws away all blank symbols on the left-most end of u and
the right-most end of v.

Definition 12.2
: yields relation <span>&#92;(\vdash_M &#92;)</span> on pairs of expressions representing configs:
1. <span>&#92;(\delta(p,a)=(q,b,\to) &#92;)</span>:
<span>&#92;[
u(p, a) c v \vdash _ {M} \gamma(u b(q, c) v) \qquad
u(p, a) \vdash _ {M} \gamma(u b(q, \sqcup))
&#92;]</span>
2. <span>&#92;(\delta(p,a)=(q,b,\gets) &#92;)</span>:
<span>&#92;[
u c(p, a) v \vdash _ {M} \gamma(u(q, c) b v) \qquad
(p, a) v \vdash _ {M} \gamma((q, \sqcup) b v)
&#92;]</span>

Also, we have <span>&#92;(\vdash _ M^* &#92;)</span> to denote  the reflexive, transitive closure of <span>&#92;(\vdash_M &#92;)</span>.

Definition 12.3
: *M* accepts, rejects *w*. *M* runs forever on input *w*.

Definition 12.4
: Let A be a language.
1. A is Turing recognizable if there exists a DTM M s.t. <span>&#92;(A=L(M) &#92;)</span>.
2. The language A is decidable if there exists a DTM M that satisfies two conditions:
    - <span>&#92;(A=L(M) &#92;)</span>, and
    - M never runs forever (either accept or reject).

## A simple example of a Turing machine
Check section 12.3 for an example of a Turing machine, which describes the language <span>&#92;(A=&#92;left&#92;{ 0^n 1^n \, : \, n\in \mathbb N &#92;right&#92;} &#92;)</span>.

# Lec 13
##  Variants of Turing machine
**DTMs allowing stationary tape heads**: add <span>&#92;(\downarrow &#92;)</span> to <span>&#92;(&#92;left&#92;{ \gets, \to &#92;right&#92;} &#92;)</span>.

**DTMs with multi-track tapes**: Specifically, we may suppose that the
tape has k tracks for some positive integer k, and for each tape head position the
tape has k separate tape squares that can each store a symbol.

**DTMs with one-way infinite tapes**:
- It is easy to simulate a DTM with a one-way infinite tape using an ordinary
DTM (with a two-way infinite tape). For instance, we could drop a special symbol,
such as ✂, on the two-way infinite tape at the beginning of the computation, to the
left of the input. If the tape head ever scans the special ✂
symbol during the computation, it moves one square right without changing state.
- Simulation of a two-way infinite tape with a
one-way infinite tape, two ways: two tracks, A special tape symbol, such as ➥, could be placed on
the first square of the bottom track to assist in the simulation; A
special symbol could be placed in the left-most square of the one-way infinite tape,
and anytime this symbol is scanned the DTM can transition into a subroutine in
which every other symbol on the tape is shifted one square to the right in order to
“make room” for a new square to the left.

**Multi-tape DTMs**:  A multi-tape DTM works in a similar way
to an ordinary, single-tape DTM, except that it has k tape heads that operate independently on k tapes.

## Encoding various objects as strings
**Encoding strings over one alphabet by strings over another**: be aware of ambiguity. For example,
```
0 -> 1     1 -> 10     2 -> 100     3 -> 1000
```

**Encoding multiple strings into one**: introduce the symbol # to indicate separation between the strings.

**Numbers, vectors, and matrices**

**Encoding graphs as binary strings**: encode adjacency matrix, or a list of edges.

# Lec 14
## Encoding for different models of computation
We want to encode things over the alphabet <span>&#92;(\Sigma &#92;)</span>.
Encode DFAs, say DFA <span>&#92;(D= (Q,\Gamma, \delta, q _ 0 , F) &#92;)</span>:
1. Denote the result (binary string) by <span>&#92;(\langle D \rangle &#92;)</span>.
2. Only assumption: Q is finite and nonempty.
3. <span>&#92;(\Gamma &#92;)</span> could be different from <span>&#92;(\Sigma &#92;)</span>. We could just assume <span>&#92;(\Gamma= &#92;left&#92;{ 0,\ldots,m-1 &#92;right&#92;} &#92;)</span>.

Encode over {0, 1, #, /} first. A hypothetical example:
<span>&#92;[
\underbrace{1011} _ {m} &#92;# \underbrace{01100 \cdots 011} _ {F} &#92;# \underbrace{0 / 0 / 110 &#92;# 0 / 1 / 1001 &#92;# \cdots &#92;# 11001 / 1010 / 011} _ {\delta}
&#92;]</span>
a/b/c (a,b,c are natural numbers): <span>&#92;(\delta(q _ a,b)=q _ c &#92;)</span>

Encoding NFA, CFG, DTM is similar.

## Simple examples of decidable languages
We denote the encoded string by <span>&#92;(\langle X,Y,\ldots, Z\rangle  &#92;)</span>. Consider the following language: A = {
<span>&#92;(\langle n, m\rangle &#92;)</span> : n and m are natural numbers that satisfy n > m }.
The problem is decidable for any reasonable
encoding scheme whatsoever. We can do two-tape DTM that decides A.

One can imagine many other languages based on the natural numbers, arithmetic,
and so on. These examples below are decidable:
```
C = {<a,b,c> : a,b,c natural num, a+b=c},
D = {<a,b,c> : a,b,c natural, ab=c},
E = {<n> : n prime}
```

We describe DTM in a high level. Take language E for an example:

On input <span>&#92;(n &#92;)</span>, where n is a natural number:
1. If  <span>&#92;(n\le 1 &#92;)</span>, reject.
2. <span>&#92;(t\gets 2 &#92;)</span>.
3. If t = n, accept.
4. If t divides n evenly, then reject.
5. <span>&#92;(t\gets t+1 &#92;)</span> and goto step 3.

Note that first line implicitly rejects all invalid inputs.

Here is one final example of a simple decidable language, which this time concerns
graph reachability. R = { <span>&#92;(\langle G, u, v\rangle &#92;)</span> :
there exists a path from u to
 v in G
}. (G undirected)

On input <span>&#92;(\langle G, u, v\rangle &#92;)</span>, where G is an undirected graph and u and v are vertices of G:
1. <span>&#92;(S\gets &#92;left&#92;{ u &#92;right&#92;} &#92;)</span>
2. <span>&#92;(F \gets 1 &#92;)</span>
3. For each vertex w of G, do:
4. &nbsp;&nbsp;&nbsp;&nbsp;  If w is not contained in S and w is adjacent to a vertex in S, then
set S ← S ∪ {w} and F ← 0.
5. If F = 0 then go to 2.
6. Accept if <span>&#92;(v\in S &#92;)</span>, otherwise reject.

## Languages concerning DFAs, NFAs, and CFGs
<span>&#92;[
A _ {DFA} = &#92;left&#92;{
    \langle D, w \rangle \, : \, D\text{ is a DFA and }w\in L(D)
     &#92;right&#92;}
&#92;]</span>

On input <span>&#92;(&#92;langle D,w &#92;rangle &#92;)</span>, where D is DFA, w string:
1. If w contains symbols not in alphabet of D, reject.
2. Simulate the computation of D on input w. If this simulation ends on an
accept state of D, then accept, and otherwise reject.

Similarly, for NFA, because it really
isn’t clear how one simulates a nondeterministic computation with a DTM. We can convert NFA to DFA first, then do the same thing. Similar for regular expressions.

A different example:
<span>&#92;[
E _ {DFA} = &#92;left&#92;{ &#92;langle D &#92;rangle\,:\, \text{D is DFA and }L(D)=\varnothing &#92;right&#92;}
&#92;]</span>

The approach is similar to the graph reachability example from before: see if there is an accept state that is reachable from the start state.


One more example:
<span>&#92;[
EQ _ {DFA}= &#92;left&#92;{ &#92;langle D,E &#92;rangle \,:\, L(D)=L(E) &#92;right&#92;}
&#92;]</span>

On input <span>&#92;(&#92;langle D,E &#92;rangle &#92;)</span>, D,E are DFAs:
1. Construct a DFA M for which it holds that L(M) = L(D) <span>&#92;(\triangle &#92;)</span> L(E).
2. If <span>&#92;(&#92;langle M &#92;rangle \in E _ {DFA} &#92;)</span>, then acept, otherwise reject.

Now let's consider <span>&#92;(A _ {CFG} &#92;)</span>.

On input <span>&#92;(&#92;langle G,w &#92;rangle &#92;)</span>,  where G is a CFG and w is a string:
1. Convert G into an equivalent CFG H in Chomsky normal form.
2. If w = ε then accept if S → ε is a rule in H and reject otherwise.
3. Search over all possible derivations by H having 2|w| − 1 steps
(of which there are finitely many). Accept if a valid derivation of
w is found, and reject otherwise.

Finally, the language E<sub>CFG</sub> can be decided using a variation on the reachability
technique. In essence, we keep track of a set containing variables that generate at
least one string, and then test to see if the start variable is contained in this set.

EQ<sub>CFG</sub> is not decidable. Some other examples of undecidable languages concerning context-free grammars
are as follows: the string is <span>&#92;(&#92;langle G &#92;rangle &#92;)</span>, and conditions are
- G is a CFG that generates all strings over its alphabet
- G is an ambiguous CFG
- G is a CFG and L(G) is inherently ambiguous

# Lec 15
## Simulating one Turing machine with another
<span>&#92;[
    A _ {DTM} = &#92;left&#92;{  &#92;left&#92;langle M,w &#92;right&#92;rangle:
        M \text{ is a DTM and }w \in L(M) &#92;right&#92;}
&#92;]</span>

Consider the language A<sub>DTM</sub>. Not decidable, but Turing recognizable. In particular we can define a DTM <span>&#92;(U &#92;)</span> (universal Turing machine)
having the following high-level description:

On input <span>&#92;(&#92;langle M,w &#92;rangle &#92;)</span>, where M is a DTM and w is a string:
1. If w is not a string over the alphabet of M, then reject.
2. Simulate M on input w. If at any point in the simulation M accepts w,
then accept; and if M rejects w, then reject.

If you have a description of a DTM M and an input string w, you can simulate M on input w by keeping track of the configuration
of M and repeatedly update it one step at a time. For the DTM U as defined above, it holds that L(U) = A<sub>DTM</sub>, and therefore
ADTM is Turing recognizable. On the other hand, U does not decide A<sub>DTM</sub> because
it might run forever on some inputs.

If we included a limitation on the number of steps a DTM is allowed to run, as part of the input, we obtain a variant of
the language defined above that is decidable:
S<sub>DTM</sub> = {<span>&#92;( &#92;langle M,w,t &#92;rangle &#92;)</span> : *M* DTM, *w* string, *t* natural num, *M* accepts *w* within *t* steps }.

This language could be decided by a DTM similar to U defined above, but where
it cuts the simulation off after t steps if M has not accepted w.

## A non-Turing-recognizable language
Define a language
<span>&#92;[
    \text{DIAG} = &#92;left&#92;{
        &#92;left&#92;langle M &#92;right&#92;rangle : M \text{ is a DTM and }
        &#92;left&#92;langle M &#92;right&#92;rangle \notin L(M)
         &#92;right&#92;}
&#92;]</span>

Over a fixed alphabet (such as the binary
alphabet). The language DIAG contains all strings that, with respect to the chosen
encoding scheme, encode a DTM that does not accept this encoding of itself.

Theorem 15.1
: The language DIAG is not Turing recognizable.

## Some undecidable languages
Proposition 15.3
: The language A<sub>DTM</sub> is undecidable.

Famous relative
<span>&#92;[
    \text{HALT} = &#92;left&#92;{
&#92;left&#92;langle M,w &#92;right&#92;rangle : M\text{ is a DTM that halts on input }w
         &#92;right&#92;}
&#92;]</span>

Let us agree that the statement “M halts on input w” is false in case w contains
symbols not in the input alphabet of M -- purely as a matter of terminology.

Easy to see HALT is Turing recognizable.

Proposition 15.4
: The language HALT is undecidable.

##  A couple of basic Turing machine tricks

### Limiting infinite search spaces
Proposition 15.5
: A, B Turing recognizable. <span>&#92;(A\cup B &#92;)</span> is Turing recognizable.

*Proof.* Define a new DTM *M* as follows:

On input w:
1. <span>&#92;(t\gets 1 &#92;)</span>
2. Run M<sub>A</sub> for t steps on w. If M<sub>A</sub> has accepted within t steps, then accept.
3. Run M<sub>B</sub> for t steps on w. If M<sub>B</sub> has accepted within t steps, then accept.
4. <span>&#92;(t\gets t+1 &#92;)</span>, and goto 2.

Using t to bound the number of steps of both simulations
is an effective way to handle this situation.



Theorem 15.7
: <span>&#92;(A, \bar A &#92;)</span> are Turing recognizable. The language A is decidable.

### Hard-coding input strings
DTM <span>&#92;(M &#92;)</span>, string <span>&#92;(x &#92;)</span>. Define <span>&#92;(M _ x &#92;)</span> as follows:

On input <span>&#92;(w &#92;)</span>: Ignore the input string <span>&#92;(w &#92;)</span> and run <span>&#92;(M &#92;)</span> on input <span>&#92;(x &#92;)</span>.

<span>&#92;[
    E _ {DTM} = &#92;left&#92;{
&#92;left&#92;langle M &#92;right&#92;rangle : M \text{ is a DTM with }L(M) = \varnothing
         &#92;right&#92;}
&#92;]</span>

Proposition 15.8
: E<sub>DTM</sub> is not decidable.

*Proof.* Assume decidable. T decides this language. Define a new DTM <span>&#92;(K &#92;)</span>:

On input <span>&#92;(&#92;left&#92;langle M,w &#92;right&#92;rangle &#92;)</span>:
1. If w is not a string over input alphabet of M, reject.
2. Compute an encoding <span>&#92;(&#92;left&#92;langle M _ w &#92;right&#92;rangle &#92;)</span>.
3. Run T on input <span>&#92;(&#92;left&#92;langle M _ w &#92;right&#92;rangle &#92;)</span>. If T accepts <span>&#92;(&#92;left&#92;langle M _ w &#92;right&#92;rangle &#92;)</span>, then reject, ow accept.

M DTM, w ∈ L(M). Consider K on input <span>&#92;(&#92;left&#92;langle M,w &#92;right&#92;rangle &#92;)</span>. <span>&#92;(M _ w &#92;)</span> accepts every string, then <span>&#92;(L(M _ w)\ne \varnothing &#92;)</span>, then T rejects <span>&#92;(&#92;left&#92;langle M _ w &#92;right&#92;rangle &#92;)</span>。

On the other hand w ∉ L(M). Similar argument, K reject <span>&#92;(&#92;left&#92;langle M _ w &#92;right&#92;rangle &#92;)</span>.

Thus K decides A<sub>DTM</sub>.

# Lec 16
##  Computable functions and reductions

Definition 16.1
: <span>&#92;(f : \Sigma ^ * \to \Sigma ^ * &#92;)</span> a function. f is *computable* if there exists a DTM M whose yields relations satisfies <span>&#92;[
    ( q _ 0, \sqcup) w \vdash _ M ^* (q _ {acc}, \sqcup) f(w)
&#92;]</span> for every string <span>&#92;(w \in \Sigma^* &#92;)</span>.

So if we run M on w, it will accept, with just the output <span>&#92;(f(w) &#92;)</span> written on the tape.

The
specific type of reduction we will consider is sometimes called a mapping reduction — but because this is the only type of reduction we will be concerned with,
we’ll stick with the simpler term reduction.

Definition 16.2
: A, B be languages. A *reduces* to B if there exists a computable function f such that
<span>&#92;(w\in A \iff f(w)\in B &#92;)</span> for all w. One writes <span>&#92;(A \le _ m B &#92;)</span> to denote A reduces B. Any such function f is called a reduction from A to B.

Theorem 16.3
: A, B be languages. <span>&#92;(A \le _ m B &#92;)</span>.
1. B decidable, then A decidable.
2. B Turing recognizable, then A Turing recognizable.

Proposition 16.4
: <span>&#92;(\le _ m &#92;)</span> is transitive.

Proposition 16.5
: <span>&#92;(A \le _ m B \iff \bar A \le _ m \bar B &#92;)</span>

##  Proving undecidability through reductions
When we prove a reduction on an
assignment or exam, we would not be expected to provide this much detail.

Example 16.6
: A<sub>DTM</sub> ⩽<sub>m</sub> HALT

Example 16.7
: DIAG ⩽<sub>m</sub> E<sub>DTM</sub>

Example 16.8
: A<sub>DTM</sub> ⩽<sub>m</sub> AE where AE = { <span>&#92;(&#92;left&#92;langle M &#92;right&#92;rangle &#92;)</span> : M is a DTM that accepts <span>&#92;(\varepsilon &#92;)</span>}.

Example 16.9
: E<sub>DTM</sub> ⩽<sub>m</sub> REG where REG =  { <span>&#92;(&#92;left&#92;langle M &#92;right&#92;rangle &#92;)</span> : M is a DTM such that L(M) is regular }

# Lec 17
## Decidable language closure properties
Proposition 17.1 & 2
: A, B decidable languages. Then <span>&#92;(A\cup B, AB, A^*,\bar A &#92;)</span> are decidable.

Example 17.3
: Prefix(A) might not be decidable, even if A is decidable.


## Turing-recognizable language closure properties
Proposition 17.4 & 5
: A, B Turing-recognizable. Then <span>&#92;(A\cup B, AB, A^*, A\cap B &#92;)</span> are Turing recognizable.


##  Nondeterministic Turing machines
Definition 17.6
: A NTM is a 7-tuple
<span>&#92;[
    N = (Q, \Sigma, \Gamma, \delta, q _ 0, q _ {acc}, q _ {rej}),
&#92;]</span>
where Q is finite & nonempty set of states; the other terms are similar as NFA + DTM.


When we think about the yields relation of an NTM N, and the way that it
computes on a given input string w, it is natural to envision a computation tree.
Note that we allow multiple nodes in the tree to represent the
same configuration, as there could be different computation paths (or paths starting
from the root node and going down the tree) that reach a particular configuration.

Theorem 17.7
: There exists an NTM *N* such that L(N) = A iff A is Turing recognizable.

## The range of a computable function
Theorem 17.8
: A be a nonempty language. The following two statements are equivalent:
1. A is Turing recognizable.
2. There exists a computable function f such that <span>&#92;(A = \operatorname{range} (f) &#92;)</span>

Corollary 17.10
: A be any infinite Turing recognizable language. Then there exists an infinite decidable language <span>&#92;(B \subseteq A &#92;)</span>.

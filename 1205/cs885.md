---
title: CS 885 - Reinforcement Learning
layout: toc
parent: Spring 2020
---

# Introduction

## Machine Learning
- Traditional computer science
    - Program computer for every task
- New paradigm
    - Provide examples to machine
    - Machine learns to accomplish a task based on the examples
        - computer vision: feed computer with lots of images, eventually learn to extract correct patterns. Programmers don't need to think of the rules to achieve right things.
        - NLP: learn to do machine translation. Discover patterns to match right expression.
- Success mostly due to supervised learning
    - Bottleneck: need lots of labeled data
- Alternatives
    - Unsupervised learning, semi-supervised learning
    - Reinforcement Learning

## What is RL?
Reinforcement learning is also known as
- Optimal control
- Approximate dynamic programming
- Neuro-dynamic programming

which are from different history or perspective.

From wiki: reinforcement learning is an area of
machine learning inspired by behavioural psychology,
concerned with how software agents ought to take
actions in an environment so as to maximize some
notion of cumulative reward.

It comes from psychology. In animal psychology,
- Negative reinforcements:
    - Pain and hunger
- Positive reinforcements:
    - Pleasure and food
- Reinforcements used to train animals

The problem:

![there should be a picture....](/pics/rl_problem.png)


**Goal**: Learn to choose actions that maximize rewards

## Examples
- Game playing (go, atari, backgammon)
- Operations research (pricing, vehicle routing)
- Elevator scheduling
- Helicopter control
- Spoken dialog systems
- Data center energy optimization
- Self-managing network systems
- Autonomous vehicles
- Computational finance

### Operations research
Historically, it was called approximate dynamical programming. Let's look at an exmaple of vehicle routing.
- **Agent**: vehicle routing software
- **Environment**: stochastic *demand*: the orders that come from customers, or needs to ship different parts
- **State**: vehicle location, capacity and depot requests
- **Action**: vehicle route
- **Reward**: - travel costs. We want to minimize the cost.

### Robotic Control
helicopter control. It is very hard to control. Naturally unstable.
- **Agent**: controller
- **Environment**: helicopter, or the air around it.
- **State**: position, orientation, velocity and angular velocity
- **Action**: collective pitch, cyclic pitch, tail rotor control
- **Reward**: - deviation from desired trajectory

2008 (Andrew Ng): automated helicopter wins acrobatic competition against humans. [Quick video](https://youtu.be/0JL04JJjocc)

### Game Playing
Go (one of the oldest and hardest board games)
- **Agent**: player
- **Environment**: opponent
- **State**: board configuration
- **Action**: next stone location
- **Reward**: +1 win / -1 lose
- 2016: AlphaGo defeats top player Lee Sedol (4-1)
    - Game 2 move 37: AlphaGo plays unexpected move (odds 1/10,000)

### Conversational agent
- **Agent**: virtual assistant
- **Environment**: user
- **State**: conversation history
- **Action**: next utterance
- **Reward**: points based on task completion, user satisfaction, etc.
- Today: active area of research

### Computational Finance
Automated trading
- **Agent**: trading software
- **Environment**: other traders
- **State**: price history
- **Action**: buy/sell/hold
- **Reward**: amount of profit

Example: how to purchase a large # of shares in a short period of time without affecting the price

Thus, RL is comprehensive, but challenging form of machine learning
- Stochastic environment
- Incomplete model
- Interdependent sequence of decisions
- No supervision
- Partial and delayed feedback

**Long term goal**: lifelong machine learning

# Markov Processes
If we unroll the problem,
- Unrolling the control loop leads to a sequence of
states, actions and rewards:
&#92;[
    s_ 0, a_ 0, r_ 0, s_ 1, a_ 1, r_ 1, s_ 2, a_ 2, r_ 2, \ldots
&#92;]
(state, action, reward)
- This sequence forms a stochastic process (due to some uncertainty in the dynamics of the process)

## Common Properties
- Processes are rarely arbitrary
- They often exhibit some structure
    - Laws of the process do not change
    - Short history sufficient to predict future

Example: weather prediction.
- Same model can be used everyday to predict weather
- Weather measurements of past few days sufficient to predict weather.

## Stochastic Process
Now consider the sequence of states only

<div class="fancy-block" data-type="Definition" data-title="Stochastic Process">
<div class="fancy-block-content">
A set of states: &#92;(S &#92;). Stochastic dynamics: &#92;(Pr(s_ t&#124;s_ {t-1},\ldots, s_ 0 ) &#92;)
</div></div>
Conditional distribution over the current state given the past states.

![there should be a picture....](/pics/sto_pro.png)

However, we might have infinitely large conditional distributions. Solutions:
- Stationary process: dynamics do not change over time
- Markov assumption: current state depends only on a finite history of past states

## K-order Markov Process
- Assumption: last k states sufficient
- First-order Markov Process: &#92;(Pr(s_ t&#124; s_ {t-1},\ldots, s_0)= Pr(s_ t &#124;s_ {t-1}) &#92;)
- Second-order Markov Process: &#92;(Pr(s_ t&#124; s_ {t-1},\ldots, s_0)= Pr(s_ t &#124;s_ {t-1}, s_ {t-2}) &#92;)

By default, a Markov Process refers to a
- First-order process for all &#92;(t &#92;)
- Stationary process: &#92;(Pr(s_ t &#124; s_ {t-1}) = Pr(s_ {t'}&#124; s_ {t'-1}) ~ \forall t'&#92;). This is a bit counterintuitive, but the dynamics is not changing.
- **Advantage**: can specify the entire process with a
single concise conditional distribution &#92;(Pr(s'&#124;s) &#92;).

**Examples**
- Robotic control
    - States: &#92;(\langle x, y, z, \theta \rangle &#92;) coordinates of joints
    - Dynamics: constant motion
- Inventory management
    - States: Inventory level
    - Dynamics: constant (stochastic) demand

## Non-Markovian and/or non-stationary processes
*What if the process is not Markovian and/or not
stationary?*

**Solution**: add new state components until dynamics
are Markovian and stationary
- Robotics: the dynamics of &#92;(\langle x, y, z, \theta \rangle &#92;) are not stationary when velocity varies…
- Solution:  add velocity to state description e.g.  &#92;(\langle x, y, z, \theta, \dot{x}, \dot{y}, \dot{z}, \dot{\theta}\rangle &#92;)
-  If acceleration varies… then add acceleration to state
- Where do we stop?

It's like Taylor series expansion.

**Problem**: adding components to the state
description to force a process to be Markovian and
stationary may significantly increase computational
complexity

**Solution**: try to find the smallest state description
that is self-sufficient (i.e., Markovian and stationary). Not sth ez to come up with.

## Inference in Markov processes
Common task is to do prediction: &#92;(Pr(s_ {t+k}&#124; s_ t) &#92;)

Computation:
&#92;[
    \operatorname{Pr}\left(s_{t+k} &#124; s_{t}\right)=\sum_{s_{t+1} \cdots s_{t+k-1}} \prod_{i=1}^{k} \operatorname{Pr}\left(s_{t+i} &#124; s_{t+i-1}\right)
&#92;]

Discrete states (matrix operations):
- Let &#92;(T &#92;) (transition matrix) be a &#92;(&#124;S&#124;\times &#124;S&#124; &#92;) matrix representing &#92;(Pr(s_ {t+1}&#124;s_ t) &#92;)
- Then &#92;(Pr (s_ {t+k}&#124; s_ t)=T^k &#92;)
- Complexity: &#92;(O(k&#124;S&#124;^3) &#92;)

However,
- Predictions by themselves are useless
- They are only useful when they will influence future decisions
- Hence the ultimate task is *decision making*
- How can we influence the process to visit desirable states?
    - Model: Markov *Decision* Process

# Markov Decision Process
Now let's augment the markov process with actions &#92;(a_ t &#92;) and rewards &#92;( r_ t &#92;).

![there should be a picture....](/pics/mdp.png)

## Current Assumptions
- Uncertainty: *stochastic* process
- Time: *sequential* process
- Observability: *fully* observable states (a bit restrictive for now)
- No learning: *complete* model
- Variable type: *discrete* (e.g., discrete states and actions)

## Rewards
is a real number &#92;(r_t\in\mathfrak R &#92;)

Reward function: &#92;(R(s_ t, a_ t)=r_ t &#92;) mapping from state-action pairs to rewards
- in some situation, it will be pretty clear like in computational finace; in some others like in conversational agents, we need to come up with some numerical signals to capture that property. With this, we will be able to design algorithm to maximize rewards.
- **Common assumption**:  stationary reward function: &#92;(R(s_ t, a_ t) &#92;) is the same &#92;(\forall t &#92;)
- **Exception**: terminal reward function often different
    - E.g., in a game: 0 reward at each turn and +1/-1 at the end for winning/losing
- Goal: &#92;(\max \sum_ t R(s_ t, a_ t) &#92;)

However, if process is infinite, isn't &#92;(\sum_ t R(s_ t, a_ t) &#92;) infinite? Two solutions
- Solution 1: **discounted rewards**
    - Discount factor: &#92;(0\le \gamma < 1 &#92;). Inflation rate... (=1 is fine if we have finite horizon)
    - Finite utility: &#92;(\sum_t \gamma^t R(s_ t,a_ t) &#92;) is a geometric sum
    - &#92;(\gamma &#92;) induces an inflation rate of &#92;(1/\gamma -1 &#92;)
    - Intuition: prefer utility sooner than later
- Solution 2: **average rewards**
    - More complicated computationally
    - Beyond the scope of this course

<div class="fancy-block" data-type="Definition" data-title="Markov Decision Process">
<div class="fancy-block-content">
<ul>
<li>Set of states: &#92;(S &#92;)</li>

<li>Set of actions: &#92;(A &#92;)</li>

<li>Transition model: &#92;(Pr(s_ t &#124; s_ {t-1}, a_ {t-1}) &#92;)</li>

<li>Reward model: &#92;(R(s_ t, a_ t) &#92;)</li>

<li>Discount factor: &#92;(0\le \gamma \le 1 &#92;)


<ul>
<li>discounted: &#92;(\gamma &lt; 1 &#92;)</li>

<li>undiscounted: &#92;(\gamma = 1 &#92;)</li></ul>
</li>

<li>Horizon (i.e., # of time steps): &#92;(h &#92;)


<ul>
<li>Finite horizon: &#92;(h\in\mathbb N &#92;)</li>

<li>Infinite horizon: &#92;(h=\infty &#92;)</li></ul>
</li>
</ul>
</div></div>

**Goal**: find optimal policy

Let's take a look at an example: Inventory management.
-  States: inventory levels
-  Actions: {doNothing, orderWidgets}
-  Transition model: stochastic demand
-  Reward model: Sales - Costs - Storage
-  Discount factor: 0.999
-  Horizon: ∞

*Tradeoff*: increasing supplies decreases odds of missed sales, but increases storage costs

## Policy
is choice of action at each time step. Formally:
- mapping from states to actions
- i.e.m &#92;(\pi(s_ t) = a_ t &#92;)
- Assumptions: fully observable states
    - Allows &#92;(a_t &#92;) to be chosen only based on current state &#92;(s_ t &#92;)

**Policy evaluation**: compute expected utility (expectation)
&#92;[
    V^{\pi}\left(s_{0}\right)=\sum_{t=0}^{h} \gamma^{t} \sum_{s_{t}} \operatorname{Pr}\left(s_{t} | s_{0}, \pi\right) R\left(s_{t}, \pi\left(s_{t}\right)\right)
&#92;]
This expression corresponds to the policy &#92;(\pi &#92;) when we start in state &#92;(s_0 &#92;)

**Optimal policy**: Policy with highest expected utility
&#92;[
    V^{\pi^* } (s_ 0) \ge V^\pi (s_0)\quad \forall \pi
&#92;]
This is the best policy &#92;(\pi^* &#92;).

# Policy Optimization
Several classes of algorithms:
- Value iteration
- Policy iteration
- Linear Programming
- Search techniques

Computation may be done
-  Offline: before the process starts, planning ahead
-  Online: as the process evolves

## Value Iteration
- Performs dynamic programming
- Optimizes decisions in reverse order

Value when no time left:
&#92;[V(s_ h) = \max_ {a_ h} R(s_ h, a_ h) &#92;]

Value with one time step left:
&#92;[
    V\left(s_ {h-1}\right)=\max _ {a_ {h-1}} R\left(s_ {h-1}, a_ {h-1}\right)+\gamma \sum_ {s_ {h}} \operatorname{Pr}\left(s_ {h} &#124; s_ {h-1}, a_ {h-1}\right) V\left(s_ {h}\right)
&#92;]

Value with two time steps left:
&#92;[
    V\left(s_ {h-2}\right)=\max _ {a_ {h-2}} R\left(s_ {h-2}, a_ {h-2}\right)+\gamma \sum_{s_ {h-1}} \operatorname{Pr}\left(s_ {h-1} &#124; s_ {h-2}, a_ {h-2}\right) V\left(s_ {h-1}\right)
&#92;]

...

Bellman's equation:
&#92;[
\begin{aligned}
V\left(s_{t}\right)&=\max _ {a_{t}} R\left(s_ {t}, a_ {t}\right)+\gamma \sum_{s_ {t+1}} \operatorname{Pr}\left(s_{t+1} &#124; s_{t}, a_{t}\right) V\left(s_{t+1}\right) &#92;&#92;
a_{t}^{* }&=\underset{a_ {t}}{\operatorname{argmax}} R\left(s_ {t}, a_ {t}\right)+\gamma \sum_{s_{t+1}} \operatorname{Pr}\left(s_{t+1} &#124; s_{t}, a_{t}\right) V\left(s_{t+1}\right)
\end{aligned}
&#92;]

Finite horizon
-  When h is finite,
-  *Non-stationary* optimal policy
-  Best action different at each time step
-  Intuition: best action varies with the amount of time left

Infinite horizon
-  When h is infinite,
-  *Stationary* optimal policy
-  Same best action at each time step
-  Intuition: same (infinite) amount of time left at each time step, hence same best action
-  **Problem**: value iteration does an infinite number of iterations…

Assuming a discount factor &#92;(\gamma &#92;), after &#92;(n &#92;) time steps, rewards are scaled down by &#92;(\gamma^n &#92;).

For large enough &#92;(n &#92;), rewards become insignificant since &#92;(\gamma^n\to 0 &#92;)

Solution:
- pick large enough &#92;(n &#92;)
- run value iteration for &#92;(n &#92;) steps
- execute policy found at the &#92;(n^{th} &#92;) iteration

### Algorithm
![there should be a picture....](/pics/val_inter_mdp.svg)

Optimal policy &#92;(\pi^* &#92;)
- &#92;(t=0 &#92;): &#92;(\pi_0^* (s) \gets \operatorname{argmax}_a R(s,a)\quad \forall s &#92;)
- &#92;(t>0 &#92;): &#92;(\pi_t^* (s)\gets \underset{a}{\operatorname{argmax}} R(s, a)+\gamma \sum_ {s^{\prime}} \operatorname{Pr}\left(s^{\prime} &#124; s, a\right) V_ {t-1}^{* }\left(s^{\prime}\right)\quad  \forall s  &#92;)

**NB**:
- &#92;(t &#92;) indicates the # of time steps to go (till end of process)
- &#92;(\pi^* &#92;) is non stationary (i.e., time dependent)

Matrix form:
- &#92;(R^a &#92;): &#92;(&#124;S &#124;\times 1 &#92;) column vector of rewards for &#92;(a &#92;)
- &#92;(V_t^* &#92;): &#92;(&#124; S &#124;\times 1 &#92;) column vector of state values
- &#92;(T^a &#92;): &#92;(&#124; S &#124; \times &#124; S &#124;&#92;) matrix of transition probability for &#92;(a &#92;)

![there should be a picture....](/pics/vi_matrix.svg)

For infinite horizon, let &#92;(h\to \infty &#92;), then &#92;(V_ h^\pi \to V_ \infty^\pi &#92;)

Policy evaluation:
&#92;[
    V_{\infty}^{\pi}(s)=R\left(s, \pi_{\infty}(s)\right)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} &#124; s, \pi_{\infty}(s)\right) V_{\infty}^{\pi}\left(s^{\prime}\right) ~~ \forall s
&#92;]

Bellman's equation:
&#92;[
    V_{\infty}^{* }(s)=\max_ {a} R(s, a)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} &#124; s, a\right) V_{\infty}^{* }\left(s^{\prime}\right)
&#92;]

Let's take a closer look at policy evaluation.
- Linear system of equations
<span>&#92;[
    V_{\infty}^{\pi}(s)=R\left(s, \pi_{\infty}(s)\right)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, \pi_{\infty}(s)\right) V_{\infty}^{\pi}\left(s^{\prime}\right)\quad \forall s
&#92;]</span>
- Matrix form:
    - <span>&#92;(R &#92;)</span>: <span>&#92;(|S|\times 1 &#92;)</span> column vector of state rewards for <span>&#92;(\pi &#92;)</span>
    - <span>&#92;(V &#92;)</span>: <span>&#92;(|S|\times 1 &#92;)</span> column vector of state values for <span>&#92;(\pi &#92;)</span>
    - <span>&#92;(T &#92;)</span>: <span>&#92;(|S|\times |S| &#92;)</span> matrix of transition prob for <span>&#92;(\pi &#92;)</span>

<span>&#92;[
    V=R+\gamma TV
&#92;]</span>

Now we want to solve linear equations. Several ways:
- Gaussian elimination: <span>&#92;((I-\gamma T) V =R&#92;)</span>
- Compute inverse: <span>&#92;(V=(I-\gamma T)^{-1}R &#92;)</span>
- Iterative methods
    - value iteration (a.k.a. Richardson iteration)
    - repeat <span>&#92;(V\gets R+\gamma T V &#92;)</span>

When we do iteration, the idea is to let the value converge. How do we know it converges?

Let <span>&#92;(H(V) \stackrel{\text { def }}{=} R+\gamma T V  &#92;)</span> be the policy eval operator.

<div class="fancy-block"  data-type="Lemma">
<div class="fancy-block-content">
    <span>&#92;(H &#92;)</span> is a contraction mapping.
    <span>&#92;[
        \|H(\tilde V)-H(V)\|_ \infty \le \gamma \| \tilde V - V\|
    &#92;]</span>
</div></div>

**Proof**:
<span>&#92;[
\begin{aligned}
|| H(\tilde{V})-H(V)|| &#95; {\infty}
&=|| R+\gamma T \tilde{V}-R-\gamma T V|| &#95; {\infty} \quad \text { (by definition) } &#92;&#92;
&=|| \gamma T(\tilde{V}-V)|| &#95; {\infty} \quad \text { (simplification) } &#92;&#92;
&\leq \gamma|| T|| &#95; {\infty}|| \tilde{V}-V|| &#95; {\infty} \quad \text { (since }&#92;| A B&#92;| \leq&#92;| A&#92;|&#92;| B&#92;|) &#92;&#92;
&=\gamma|| \tilde{V}-V|| &#95; {\infty} \quad \text { (since } \max &#95; {s} \sum &#95; {s^{\prime}} T\left(s, s^{\prime}\right)=1)
\end{aligned}
&#92;]</span>

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
     Policy evaluation converges to <span>&#92;(V^\pi &#92;)</span> for any initial estimate <span>&#92;(V &#92;)</span>
     <span>&#92;[
         \lim_ {n\to\infty} H^{(n)}(V) = V^\pi \quad \forall V
     &#92;]</span>
</div></div>

Proof omitted or see slides...

In practice, we can’t perform an infinite number
of iterations. Suppose that we perform value iteration for <span>&#92;(n &#92;)</span>
steps and <span>&#92;(\|H^{(n)}(V)-H^{(n-1)}(V)\|_ \infty=\epsilon &#92;)</span>, how far is <span>&#92;(H^{(n)}(V) &#92;)</span> from <span>&#92;(V^\pi &#92;)</span>?

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    If <span>&#92;(\|H^{(n)}(V)-H^{(n-1)}(V)\|_ \infty\le\epsilon &#92;)</span> then
    <span>&#92;[
        \|V^\pi - H^{(n)}(V)\|_ \infty \le {\epsilon \over 1-\gamma}
    &#92;]</span>
</div></div>

Proof omitted or see slides...

Now let's take a closer look at *optimal value function*
- Non-linear system of equations
<span>&#92;[
    V_ {\infty}^{* }(s)=\max _ {a} R(s, a)+\gamma \sum _ {s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, a\right) V _ {\infty}^{* }\left(s^{\prime}\right)\quad \forall s
&#92;]</span>
- Matrix form
    - <span>&#92;(R^a &#92;)</span>: <span>&#92;(|S|\times 1 &#92;)</span> column vector of state rewards for <span>&#92;(a &#92;)</span>
    - <span>&#92;(V^* &#92;)</span>: <span>&#92;(|S|\times 1 &#92;)</span> column vector of state values for optimal values
    - <span>&#92;(T^a &#92;)</span>: <span>&#92;(|S|\times |S| &#92;)</span> matrix of transition prob for <span>&#92;(a &#92;)</span>

<span>&#92;[
    V^* = \max_ a R^a + \gamma T^a V^*
&#92;]</span>

Let <span>&#92;(H^* \stackrel{\text { def }}{=} \max_a R^a + \gamma T^a V &#92;)</span> be the operator in value iteration

<div class="fancy-block"  data-type="Lemma">
<div class="fancy-block-content">
    <span>&#92;(H^* &#92;)</span> is a contraction mapping.

    <span>&#92;[
        \|H^* (\tilde V) - H^* (V)\|_ \infty \le \gamma \|\tilde V - V \|_ \infty
    &#92;]</span>
</div></div>

Proof omitted.

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    Value iteration converges to <span>&#92;(V^* &#92;)</span> for any initial estimate <span>&#92;(V &#92;)</span>
    <span>&#92;[
        \lim_ {n\to\infty } {H^* }^{(n)} (V) = V^* \quad \forall V
    &#92;]</span>
</div></div>

Proof skipped.

Even when horizon is infinite, we perform finitely many iterations. Stop when <span>&#92;(\|V_ n- V_ {n-1}\|_ \infty \le \epsilon &#92;)</span>.

Since <span>&#92;(\|V_ n- V_ {n-1}\|_ \infty \le \epsilon &#92;)</span>, by Theorem 7, we know that <span>&#92;(\|V_ n- V^* \|_ \infty \le {\epsilon \over 1-\gamma} &#92;)</span>. But, how good is the stationary policy <span>&#92;(\pi_n(s) &#92;)</span> extracted based on <span>&#92;(V_n &#92;)</span>?

<span>&#92;[
    \pi_{n}(s)=\underset{a}{\operatorname{argmax}} R(s, a)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, a\right) V_{n}\left(s^{\prime}\right)
&#92;]</span>

How far is <span>&#92;(V^{\pi_ n} &#92;)</span> from <span>&#92;(V^* &#92;)</span>?

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    <span>&#92;[
        \&#124;V^{\pi_ n} - V^* \&#124;_ \infty \le {2\epsilon \over 1-\gamma}
    &#92;]</span>
</div></div>

Proof skipped.

In summary, value iteration is simple DP algorithm with complexity <span>&#92;(O (n |A| |S|^2) &#92;)</span> where <span>&#92;(n &#92;)</span> is the number of iterations.
- Optimize value function
- Extract induced policy

Can we optimize the policy directly instead of
optimizing the value function and then inducing a
policy?
- Yes: by policy iteration

## Policy Iteration
It alternates between two steps
- Policy evaluation
<span>&#92;[
    V^{\pi}(s)=R(s, \pi(s))+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, \pi(s)\right) V^{\pi}\left(s^{\prime}\right) \quad \forall s
&#92;]</span>
- Policy improvement
<span>&#92;[
    \pi(s) \leftarrow \underset{a}{\operatorname{argmax}} R(s, a)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, a\right) V^{\pi}\left(s^{\prime}\right) \quad \forall s
&#92;]</span>

until convergence.

![there should be a picture....](/pics/pol_iter.svg)



<div class="fancy-block"  data-type="Lemma" data-title="Monotonic Improvement">
<div class="fancy-block-content">
    Let <span>&#92;(V_n &#92;)</span> and <span>&#92;(V_{n+1} &#92;)</span> be successive value functions in policy iteration. Then <span>&#92;(V_ {n+1}\ge V_ n &#92;)</span>
</div></div>


<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    Policy iteration converges to <span>&#92;(\pi^* ~\&~V^* &#92;)</span> in finitely many iterations when <span>&#92;(S &#92;)</span> and <span>&#92;(A &#92;)</span> are finite.
</div></div>

Let's do a comparison:
- Value Iteration:
    - Each iteration: <span>&#92;(O(|S|^2 |A|) &#92;)</span>
    - Many iterations: linear convergence
- Policy Iteration
    - Each iteration: <span>&#92;(O(|S|^3 + |S|^2 |A|) &#92;)</span>
    - Few iterations: linear-quadratic convergence, which converges faster

Ideally, we would like a technique which has fewer iterations and smaller cost per iteration.
### Modified Policy Iteration
alternates between two steps:
1. **Partial** Policy evaluation, repeat <span>&#92;(k &#92;)</span> times
2. Policy improvement

![there should be a picture....](/pics/modified_pol_inter.png)

Same convergence guarantees as value iteration. Proof: somewhat complicated (see Section 6.5
of Puterman’s book)

Let's do a comparison:
- Value Iteration:
    - Each iteration: <span>&#92;(O(|S|^2 |A|) &#92;)</span>
    - Many iterations: linear convergence
- Policy Iteration
    - Each iteration: <span>&#92;(O(|S|^3 + |S|^2 |A|) &#92;)</span>
    - Few iterations: linear-quadratic convergence, which converges faster
- Modified Policy Iteration:
    - Each iteration: <span>&#92;(O(k|S|^2 + |S|^2 |A|)  &#92;)</span>
    - Few iterations: linear-quadratic convergence

# Intro to RL
Recall MDP, here we are more general in transition model and reward model:
- Transition model: <span>&#92;(Pr(s_ t| s_ {t-1}, a_ {t-1}) &#92;)</span>
- Reward model: <span>&#92;(Pr(r_ t| s_ t, a_ t) &#92;)</span>

which are stochastic instead of being deterministic.

**Goal**: find optimal policy <span>&#92;(\pi^* &#92;)</span> such that
<span>&#92;[
    \pi^* =\operatorname{argmax}_ \pi \sum _ {t=0}^h \gamma^t E _ \pi [r_ t]
&#92;]</span>

So we can define RL as follows: same thing with MDP, but where we remove transition and reward model (unknown model).

In RL, we are going to *learn an optimal policy while interacting with the
environment*.

## Example: Inverted Pendulum

![there should be a picture....](/pics/inverted_pendulum.png)

- State: <span>&#92;(x(t), x'(t), \theta(t), \theta'(t) &#92;)</span>
- Action: force <span>&#92;(F &#92;)</span>
- Reward: 1 for any step where pole balanced

Problem: Find <span>&#92;(\pi: S\to A &#92;)</span> that maximizes rewards

## Important Components in RL
RL agents may or may not include the following
components:
- Model: Environment dynamics and rewards
- Policy: Agent action choices
- Value function: Expected total rewards of the agent policy

Thus, by this, we can categorize RL agents

- **Value based**: <span style="color:gray">(No policy, implicit.)</span> Value function. If you have a value function, you can always induce a policy just by taking one step of value iteration, and noticing what is the action maximize that step.
- **Policy based**: Policy, <span style="color:gray">No value function</span>. We can use the rewards given by the environment.
- **Actor critic**: Policy, value function.

or another way to categorize:
- **Model based**: Transition and reward model
- **Model free**: <span style="color:gray">No transition and no
reward model (implicit)</span>

## Model free evaluation
Given a policy <span>&#92;(\pi &#92;)</span>, estimate <span>&#92;(V^\pi(s) &#92;)</span> without any transition or reward model.

- Monte Carlo evaluation (sample approximation)
<span>&#92;[
    V^\pi(s) = E_ \pi \left[\sum_ t \gamma^t r_ t\right] \approx {1\over n(s)} \sum_ {k=1}^{n(s)} \left[\sum_ t \gamma^t r_ t^{(k)}\right]
&#92;]</span>
- Temporal difference (TD) evaluation (one sample approximation)
<span>&#92;[
\begin{aligned}
V^{\pi}(s) &=E[r | s, \pi(s)]+\gamma \sum_ {s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, \pi(s)\right) V^{\pi}\left(s^{\prime}\right) &#92;&#92;
& \approx r+\gamma V^{\pi}\left(s^{\prime}\right)
\end{aligned}
&#92;]</span>

### Monte Carlo Evaluation
Let's look more closely at **Monte Carlo Evaluation**.

Let <span>&#92;(G_k &#92;)</span> be a one-trajectory Monte Carlo target
<span>&#92;[
    G_ k = \sum_ t\gamma^t r&#95;t^{(k)}
&#92;]</span>

Approximate value function
<span>&#92;[
\begin{aligned}
V&#95;{n}^{\pi}(s) & \approx \frac{1}{n(s)} \sum&#95;{k=1}^{n(s)} G&#95;{k} &#92;&#92;
&=\frac{1}{n(s)}\left(G&#95;{n(s)}+\sum&#95;{k=1}^{n(s)-1} G&#95;{k}\right) &#92;&#92;
&=\frac{1}{n(s)}\left(G&#95;{n(s)}+(n(s)-1) V&#95;{n-1}^{\pi}(s)\right) &#92;&#92;
&=V&#95;{n-1}^{\pi}(s)+\frac{1}{n(s)}\left(G&#95;{n(s)}-V&#95;{n-1}^{\pi}(s)\right)
\end{aligned}
&#92;]</span>

Incremental update
<span>&#92;[
    V_ n^\pi(s)\gets V&#95;{n-1}^{\pi}(s)+\underbrace{\alpha_ n}_ {\text{learning rate } 1/n(s)}\left(G&#95;{n(s)}-V&#95;{n-1}^{\pi}(s)\right)
&#92;]</span>

### Temporal Difference Evaluation
Then look at **Temporal Difference Evaluation**. Its approximate value function is
<span>&#92;[
    V^\pi (s) \approx r+ \gamma V^\pi(s')
&#92;]</span>

Incremental update:
<span>&#92;[
    V&#95;{n}^{\pi}(s) \leftarrow V&#95;{n-1}^{\pi}(s)+\alpha&#95;{n}\left(r+\gamma V&#95;{n-1}^{\pi}\left(s^{\prime}\right)-V&#95;{n-1}^{\pi}(s)\right)
&#92;]</span>

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    If <span>&#92;(\alpha_n &#92;)</span> is approximately decreased with number of times a state is visited then <span>&#92;(V_ n^\pi(s) &#92;)</span> converges to correct value.
</div></div>

Sufficient conditions for <span>&#92;(\alpha_n &#92;)</span>
- <span>&#92;(\sum_ n\alpha_ n \to\infty  &#92;)</span>
- <span>&#92;(\sum_ n(\alpha_ n)^2< \infty &#92;)</span>

Often <span>&#92;(\alpha_ n(s)= 1/n(s) &#92;)</span>, where <span>&#92;(n(s) = &#92;)</span> # of times <span>&#92;(s &#92;)</span> is visited.

![there should be a picture....](/pics/tdeval.png)


**Comparison**
- Monte Carlo evaluation:
    - Unbiased estimate
    - High variance
    - Needs many trajectories
- Temporal difference evaluation:
    - Biased estimate
    - Lower variance
    - Needs less trajectories

## Model Free Control
Instead of evaluating the state value function, <span>&#92;(V^\pi(s) &#92;)</span>, evaluate the state-action value function, <span>&#92;(Q^\pi(s,a) &#92;)</span>
- it is value of executing <span>&#92;(a &#92;)</span> followed by <span>&#92;(\pi &#92;)</span>
- <span>&#92;(Q^{\pi}(s, a)=E[r | s, a]+\gamma \sum_ {s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, a\right) V^{\pi}\left(s^{\prime}\right) &#92;)</span>

Greedy policy <span>&#92;(\pi' &#92;)</span>: <span>&#92;(\pi'(s)=\operatorname{argmax}_ aQ^\pi(s,a) &#92;)</span>

Based on this, we can define Bellman's eq w.r.t. Q function
- Optimal state value function
<span>&#92;[
    V^{* }(s)=\max &#95;{a} E[r | s, a]+\gamma \sum&#95;{s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, a\right) V^{* }\left(s^{\prime}\right)
&#92;]</span>
- Optimal state-action value function
<span>&#92;[
    Q^{* }(s, a)=E[r | s, a]+\gamma \sum&#95;{s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, a\right) \max &#95;{a^{\prime}} Q^{* }\left(s^{\prime}, a^{\prime}\right)
&#92;]</span>
where <span>&#92;(V^* (s) = \max_a Q^* (s,a ) &#92;)</span> and <span>&#92;(\pi^* (s) =\operatorname{argmax}_ a Q^* (s,a)  &#92;)</span>.

The main difference is we move max from the beginning to the end of the eqn, but that doesn't change the math really. These two eqns are mathematically equivalent.

Monte Carlo Control

![there should be a picture....](/pics/mcc.png)

Temporal Difference Control

![there should be a picture....](/pics/tdc.png)

## Q learning
![there should be a picture....](/pics/qlearning.png)

but how we select action?
## Exploration vs Exploitation
- If an agent always chooses the action with the highest value then it is **exploiting**
    - The learned model is not the real model
    - Leads to suboptimal results, might stuck to a local optimum
- By taking random actions (pure **exploration**) an agent may learn the model
    - But what is the use of learning a complete model if parts of it are never used?
    - Need a balance between exploitation and exploration

Common exploration methods
- <span>&#92;(\epsilon &#92;)</span>-greedy
    - with probability <span>&#92;(\epsilon &#92;)</span> execute random action
    - Otherwise execute best action <span>&#92;(a^* = \operatorname{argmax}_ a Q(s,a)&#92;)</span>
- Boltzmann exploration
<span>&#92;[
    \operatorname{Pr}(a)=\frac{e^{\frac{Q(s, a)}{T}}}{\sum_ {a} e^{\frac{Q(s, a)}{T}}}
&#92;]</span>

Q-learning converges to optimal Q-values if
- Every state is visited infinitely often (due to
exploration)
- The action selection becomes greedy as time approaches infinity
- The prob of exploration <span>&#92;(\epsilon &#92;)</span> is decreased fast enough, but not too fast (similar like learning rate <span>&#92;(\alpha &#92;)</span> previously)

# Deep Neural Networks
Like in computer go, we have large state spaces.

Functions to be approximated:
- Policy: <span>&#92;(\pi(s)\to a &#92;)</span>
- Q-function: <span>&#92;(Q(s,a)\in\mathfrak R &#92;)</span>
- Value function: <span>&#92;(V(s)\in\mathfrak R &#92;)</span>

We need to approximate them and we can't use tables or vectors for them. Let's focus on Q-function. In the future lectures, we might focus other functions afterwards.

Let <span>&#92;(s=(x_ 1, x_ 2,\ldots, x_ n)^T &#92;)</span>, a set/vector of features. Then we take a linear combination with weights:
<span>&#92;[
    Q(s,a) \approx \sum_ i w _ {ai}x _ i
&#92;]</span>

or non-linear (e.g., neural network): <span>&#92;(Q(s,a)\approx g({\bf x}; {\bf w}) &#92;)</span>

For the rest of this part, please consult [lecture 4a](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-lecture4a.pdf).


## Mitigating Vanishing Gradients
Some popular solutions:
- Pre-training
- Rectified linear units
- Batch normalization
- Skip connections

## Rectified Linear Units
Rectified linear: <span>&#92;(h(a)=\max(0,a) &#92;)</span>
- gradient is 0 or 1
- sparse computation since not smooth

Soft version ("Softplus"): <span>&#92;(h(a)=\log (1+e^a) &#92;)</span>

<span style="color:red">Warning</span>:  softplus
does not prevent gradient vanishing (gradient < 1)

So softplus looks like a good idea, but it's not, didn't mitigate gradient vanishing problem. So you really want to use rectified linear units. Not smooth is not a big deal, because you can use subgradient techniques to do the optimization.

# Deep Q-networks
Q-function Approximation
- Let <span>&#92;(s=(x_ 1, \ldots, x_ n)^T &#92;)</span>
- Linear <span>&#92;(Q(s, a) \approx \sum_ i w_ {ai} x_ i &#92;)</span>
- Non-linear (e.g., neural network): <span>&#92;(Q(s,a) \approx g({\bf x}; {\bf w}) &#92;)</span>

## Gradient Q-learning
Minimize squared error between Q-value estimate
and target
- Q-value estimate: <span>&#92;(Q_{\bf w}(s,a) &#92;)</span>
- Target: <span>&#92;(r+\gamma \max_ {a'} Q _ {\overline{\bf w}}(s', a') &#92;)</span>

Squared error:
<span>&#92;[
    Err({\bf w}) = {1\over 2} \left[Q_ {\bf w}(s, a) -r - \gamma \max_ {a'} Q_ {\overline {\bf w}} (s', a') \right]^2
&#92;]</span>

where <span>&#92;(\overline {\bf w} &#92;)</span> is fixed.

Gradient:
<span>&#92;[
    {\partial Err \over \partial {\bf w}} = \left[ Q_ {\bf w}(s, a) - r - \gamma \max_ {a'} Q_ { \overline {\bf w}} (s', a')\right] {\partial Q_ {\bf w}(s,a)\over \partial {\bf w}}
&#92;]</span>

![there should be a picture....](/pics/gradient-q-learning.png)

Historically, there was just one network, and we didn't use <span>&#92;(\overline{\bf w} &#92;)</span>. It sometimes works, sometimes diverges. Now we use two networks, one for main estimate, one for target. Keep target network fixed and simply update the other network, then it will get rid of the problem.

We find that even if we don't fix **w**, if we satisfy the conditions, it will converge.

Even when the following conditions hold, non-linear gradient Q-learning may diverge. The fact is that we didn't fix **w** then the target is shifting, then not be able to converge.

Intuition: adjusting **w** to increase <span>&#92;(Q &#92;)</span> at <span>&#92;((s,a) &#92;)</span> might introduce errors at nearby state-action pairs.

## Mitigating divergence
Two tricks are often used in practice

1. Experience replay
2. Use two networks:
    - Q-network
    - Target network

### Experience Replay
**Idea**: store previous experiences <span>&#92;(s,a,s',r &#92;)</span> into a
buffer and sample a mini-batch of previous
experiences at each step to learn by Q-learning

Advantages
- Break correlations between successive updates (more
stable learning)
- Fewer interactions with environment needed to converge
(greater data efficiency)

### Target Network
**Idea**: Use a separate target network that is updated only periodically

repeat for each <span>&#92;(s,a,s',r &#92;)</span> in mini-batch:
<span>&#92;[
    &#92;begin{aligned}
    {\bf w}&\gets {\bf w} -\alpha_t \left[\underbrace{Q_ {\bf w}} _ {update} - r - \gamma \max _ {a'} \underbrace{ Q_ {\overline {\bf w}} (s',a')} _ {target} \right]
    {\partial Q_ {\bf w} (s,a) \over \partial {\bf w}}&#92;&#92;
    \overline {\bf w} &\gets {\bf w}
    &#92;end{aligned}
&#92;]</span>

Advantage: mitigate divergence

It is similar to value iteration: repeat for all <span>&#92;(s &#92;)</span>. We have contraction mapping which guarantees it to converge.

## Deep Q-network
Proposed by Google Deep Mind. It is Gradient Q-learning with
- Deep neural networks
- Experience replay
- Target network

Breakthrough: human-level play in many Atari video games

![there should be a picture....](/pics/deep-q-net.png)

# Policy Gradient Methods
- Model-free policy-based method
- No explicit value function representation

Ezist policy is **Stochastic Policy**. Consider stochastic policy <span>&#92;(\pi_ \theta (a| s)= Pr(a|s;\theta ) &#92;)</span> parametrized by <span>&#92;(\theta &#92;)</span>. Continuous function.

Finitely many discrete actions. *Softmax*, quite popular in neural networks.

Softmax: <span>&#92;(\pi_ \theta (a| s) = {\exp{h(s,a;\theta)}\over \sum_ {a'} \exp{h(s,a';\theta)}} &#92;)</span> where <span>&#92;(h(s,a;\theta) &#92;)</span> might be (we can think of it as a scoring function)
- **linear** in <span>&#92;(\theta &#92;)</span>: <span>&#92;(h(s,a;\theta)=\sum_ i\theta _ i f _ i (s,a) &#92;)</span>. <span>&#92;(f_ i &#92;)</span>'s are basis functions.
- or **non-linear** in <span>&#92;(\theta &#92;)</span>: <span>&#92;(h(s,a;\theta) = neuralNet(s,a;\theta) &#92;)</span>.

Continuous actions: a distribution.
- Gaussian: <span>&#92;(\pi_ \theta (a|s) = N(a| \mu (s;\theta), \sum (s;\theta)) &#92;)</span>

Before policy gradients, take a step back, look at **supervised learning**. Consider a stochastic policy <span>&#92;(\pi_ \theta (a|s) &#92;)</span>. We have data: state-action pairs where actions are optimal for each state:
<span>&#92;[
    &#92;left&#92;{ (s_ 1, a_ 1^* ), (s _ 2, a _ 2 * ),\ldots &#92;right&#92;}
&#92;]</span>
Then we maximize log likelihood of the data:
<span>&#92;[
    \theta^* = \operatorname{argmax}_ \theta \sum _ n \log  \pi _ \theta ( a_ n ^ * | s _ n)
&#92;]</span>
and gradient update:
<span>&#92;[
    \theta _ {n+1} \gets \theta _ n + \alpha _ n \nabla _ \theta \log \pi _ \theta ( a_ n ^ * | s _ n)
&#92;]</span>

Let's compare that with RL. We will have a stochastic policy <span>&#92;(\pi _ \theta (a|s) &#92;)</span>. The difference is we don't know which is the best one for every state. Instead, we have state-action-reward triples: the action we have tried and the reward, and we don't know if it's the best one.
<span>&#92;[
    &#92;left&#92;{ (s _ 1, a _ 1, r _ 1),\ldots &#92;right&#92;}
&#92;]</span>
So we try to maximize discounted sum of rewards, expectation of future rewards:
<span>&#92;[
    \theta^* = \operatorname{argmax}_ \theta \sum _ n \gamma ^ n E _ \theta [r _ n| s _ n, a _ n]
&#92;]</span>
Gradient update:
<span>&#92;[
    \theta _ {n+1}\gets \theta _ n + \alpha _ n \gamma ^ n G _ n \nabla _ \theta \log \pi _ \theta ( a _ n | s _ n)
&#92;]</span>
where <span>&#92;(G_ n =\sum _ {t=0}^\infty \gamma^t r _ {n+t} &#92;)</span>. Monte Carlo estimate for the sum of discounted reward for one trajectory.

It looks like supervised learning, but it is hard to derive. To derive this, we introduce a theorem.

<div class="fancy-block" data-type="Theorem" data-title="Stochastic Gradient Policy Theorem">
<div class="fancy-block-content">
<span>&#92;[
\nabla V&#95;{\theta}\left(s&#95;{0}\right) \propto \sum&#95;{s} \mu&#95;{\theta}(s) \sum&#95;{a} \nabla \pi&#95;{\theta}(a | s) Q&#95;{\theta}(s, a)
&#92;]</span>
</div></div>

<span>&#92;(\mu _ \theta (s) &#92;)</span>: stationary state distribution when executing policy parametrized by <span>&#92;(\theta &#92;)</span>.

<span>&#92;(Q_ \theta (s,a) &#92;)</span>: discounted sum of rewards when starting <span>&#92;(s &#92;)</span>,
executing <span>&#92;(a &#92;)</span> and following the policy parametrized by <span>&#92;(\theta &#92;)</span>
thereafter.

Derivation skipped...

Based on this theorem, we can drop the summation, and approximation by samples (Monte Carlo Policy Gradient).
<span>&#92;[
    \nabla V _ \theta \propto \ldots = E _ \theta [\gamma ^n G_ n \nabla \log \pi _ \theta ( A _ n | S _ n)]
&#92;]</span>
When we drop expectation, we can get one sample approximation, which corresponds to Stochastic gradient descent: <span>&#92;(\nabla V _ \theta \approx \gamma ^n G_ n \nabla \log \pi _ \theta ( A _ n | s _ n) &#92;)</span>.

![this should..](/pics/rein_stoch.png)

This alg is quite old, one of the early RL techniques.

Computer Go. Winning strategy, four steps:
1. Supervised Learning of Policy Networks
2. Policy gradient with Policy Networks
3. Value gradient with Value Networks
4. Searching with Policy and Value Networks

[The rest of slides...](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-lecture7a.pdf)

# Actor Critic Algorithms
- Q-learning
    - Model-free value-based method
    - No explicit policy representation
- Policy gradient
    - Model-free policy-based method
    - No explicit value function representation
- Actor Critic
    - Model-free policy and value based method

<div class="fancy-block" data-type="Theorem" data-title="Equivalent Stochastic Gradient Policy Theorem">
<div class="fancy-block-content">
 with a baseline <span>&#92;(b(s) &#92;)</span>
<span>&#92;[
\nabla V&#95;{\theta}\left(s&#95;{0}\right) \propto \sum&#95;{s} \mu&#95;{\theta}(s) \sum&#95;{a} \nabla \pi&#95;{\theta}(a | s) [Q&#95;{\theta}(s, a) -b(s)]
&#92;]</span>
</div></div>
since <span>&#92;(\sum _ a \pi _ \theta (a|s)b(s) = b(s)\nabla \sum _ a \pi _ \theta (a | s) = b(s)\nabla 1 = 0 &#92;)</span>. This result is quite amazing, and what's the advantage of doing so?

Baseline often chosen to be <span>&#92;(b(s)\approx V^\pi (s) &#92;)</span>.

Advantage function: <span>&#92;(A(s,a) = Q(s,a) - V^\pi (s) &#92;)</span>. How much Q function differs from the value function?

Gradient update:
<span>&#92;[
    \theta \gets \theta \alpha \gamma^n A (s_ n, a_ n) \nabla \log \pi_ \theta (a _ n | s _ n)
&#92;]</span>

**Benefit**: faster empirical convergence

![pic](/pics/ri-baseline.png)

If we look at the code, we now have both a value function and a policy.

Instead of updating <span>&#92;(V(s) &#92;)</span> by Monte Carlo sampling, where we need to wait for a complete episode and get entire trajectory, not sample efficient. What is better, we can do bootstrap with temporal difference updates. So we don't need to wait until the end.
<span>&#92;[
    \delta \gets r_ n + \gamma V_ w(s _ {n+1})- V_ w(s _ n)
&#92;]</span>

**Benefit**: reduced variance (faster convergence)

This leads actor critic.

![there should be a image...](/pics/act-cri.png)

Instead of doing temporal difference updates, update with the advantage function (see algorithm below)

**Benefit**: faster convergence

This leads to Advantage Actor Critic (A2C). Also [Asynchronous Advantage Actor Critic (A3C) algorithm](https://www.geeksforgeeks.org/asynchronous-advantage-actor-critic-a3c-algorithm/). It turns out asynchronous fashion is not critical...

![there should be a image...](/pics/a2c.png)



Continuous actions. Now we have discussed stochastic policy. What if we want a policy that is deterministic? Consider a deterministic policy <span>&#92;(\pi_ \theta (s)\to a &#92;)</span>

<div class="fancy-block" data-type="Theorem" data-title="Deterministic Gradient Policy Theorem">
<div class="fancy-block-content">
    <span>&#92;[
        \nabla V&#95;{\theta}\left(s&#95;{0}\right) \propto E&#95;{s \sim \mu&#95;{\theta}(s)}\left[\left.\nabla&#95;{\theta} \pi&#95;{\theta}(s) \nabla&#95;{a} Q&#95;{\theta}(s, a)\right|&#95;{a=\pi&#95;{\theta}(s)}\right]
    &#92;]</span>
</div></div>

Proof: see Silver et al. 2014

So this leads to Deterministic Policy Gradient. In some literatures, we have DDPG (deep ...) which is encoded in neural networks. This is actually actor-critic technique, with explicit Q-function, policy and we do updates for both of them.

![there should be a image...](/pics/DPG.png)

# Trust Region Methods

It is common to formulate ML problems as
optimization problems.
- Min squared error
- Min cross entropy
- Max log likelihood
- Max discounted sum of rewards

There are two important classes:
- Line search methods
    - Find a direction of improvement
    - Select a step length
- Trust region methods
    - Select a trust region (analog to max step length)
    - Find a point of improvement in the region

Idea: approximate obj <span>&#92;(f &#92;)</span> with a simpler obj <span>&#92;(\tilde f &#92;)</span> and solve <span>&#92;(\tilde x^* = argmin_x \tilde f(x) &#92;)</span>

<span style="color:red">Problem: </span> The optimum <span>&#92;(\tilde x^* &#92;)</span> might be in a region where <span>&#92;(\tilde f &#92;)</span> poorly approximates <span>&#92;(f &#92;)</span> and <span>&#92;(\tilde x^* &#92;)</span> might be far from optimal

<span style="color:green">Solution: </span> restrict the search to a region where we trust <span>&#92;(\tilde f &#92;)</span>
to approximate <span>&#92;(f &#92;)</span> well.

For example, <span>&#92;(\tilde f &#92;)</span> often chosen to be a quadratic approx of <span>&#92;(f &#92;)</span>

<span>&#92;[
    &#92;begin{aligned}
    f(x)&\approx \tilde f(x) &#92;&#92;
    &= f(c) + \nabla f(c)^T (x-c) + {1\over 2!}(x-c)^T H(c)(x-c)
    &#92;end{aligned}
&#92;]</span>
where <span>&#92;(\nabla f &#92;)</span> is the gradient and <span>&#92;(H &#92;)</span> is the hessian. Trust region often chosen to be a hypersphere <span>&#92;(&#124;&#124;x-c &#124;&#124;_ 2\le \delta  &#92;)</span>

Generic algorithm: alternate between solving opt (trust region subproblem) and adjusting region size. Repeat until convergence.

When <span>&#92;(H &#92;)</span> is positive semi-definite, we get a convex opt, which gives us simple and globally optimal solution. When <span>&#92;(H &#92;)</span> is not PSD, it's non-convex optimization, simple heuristics that guarantee improvement.

# Trust Region Policy Optimization
Recall policy gradient, <span>&#92;(\alpha &#92;)</span> (learning rate) is difficult to set:
- small: slow but reliable convergence
- big: fast but reliable

## Trust region method
We	often	optimize	a
surrogate (替代)	objective
(approximation	of	<span>&#92;(V &#92;)</span>). Surrogate	objective	may
be	trustable	(close	to	<span>&#92;(V &#92;)</span>)
only	in	a	small	region. <span style="color:red">Limit search to small trust region</span>.

Let <span>&#92;(\theta &#92;)</span> be the parameters for policy <span>&#92;(\pi _ \theta (s|a) &#92;)</span>. We can define a region
around <span>&#92;(\theta &#92;)</span> or around <span>&#92;(\pi_ \theta &#92;)</span>. <span>&#92;(V &#92;)</span> often varies more smoothly with <span>&#92;(\pi_ \theta &#92;)</span> than <span>&#92;(\theta &#92;)</span>. Hence define <span style="color:green">policy trust regions</span>.

Kullback-Leibler Divergence. Intuition: expectation of the log diff between <span>&#92;(p &#92;)</span> and <span>&#92;(q &#92;)</span>,

<span>&#92;[
    D _ {K L}(p, q)=\sum _ {x} p(x) \log \frac{p(x)}{q(x)}
&#92;]</span>

Trust region policy optimization. Update step:
<span>&#92;[
\begin{aligned}
\theta \leftarrow & \underset{\widetilde{\theta}}{\operatorname{argmax}} E&#95;{s&#95;{0} \sim p}\left[V^{\pi&#95;{\tilde{\theta}}}\left(s&#95;{0}\right)-V^{\pi&#95;{\theta}}\left(s&#95;{0}\right)\right] &#92;&#92;
& \text { subject to } \max &#95;{\mathrm{s}} D&#95;{K L}\left(\pi&#95;{\theta}(\cdot \mid s), \pi&#95;{\widetilde{\theta}}(\cdot \mid s)\right) \leq \delta
\end{aligned}
&#92;]</span>
Since the objective is not directly computable, let’s
approximate it:
<span>&#92;[
    \underset{\widetilde{\theta}}{\operatorname{argmax}} E&#95;{s&#95;{0} \sim p}\left[V^{\pi&#95;{\tilde{\theta}}}\left(s&#95;{0}\right)-V^{\pi&#95;{\theta}}\left(s&#95;{0}\right)\right]
    \approx
    \underset{\widetilde{\theta}}{\operatorname{argmax}} E_ {s\sim \mu _ \theta, a \sim \pi _ \theta} \left[{\pi _ {\tilde\theta (a | s)\over \pi _ \theta ( a| s)}} A _ \theta (s,a) \right]
&#92;]</span>
where <span>&#92;(\mu_ \theta (s) &#92;)</span> is the stationary state distribution for <span>&#92;(\pi &#92;)</span>.

Let's also relax the bound on the max KL-divergence
to a bound on the expected KL-divergence, which is relaxed to expectation
<span>&#92;[
    E_ {s\sim \mu _ \theta} [D _ {KL} \left( \pi _ \theta (\cdot | s), \pi _ {\tilde \theta} ( \cdot | s) \right)]
&#92;]</span>

Derivation skipped.

Trust Region Policy Optimization (TRPO). Most of the algorithm is identical to actor-critic. Difference is update to the actor as we mentioned before.

One drawback is the complexity of optimization. In practice, obj is approximated with linear function and constraint is approximated with a quadratic function.

TRPO is conceptually and computationally
challenging in large part because of the constraint in
the optimization.
The effect of the constraint: effectively constraining the ratio <span>&#92;({\pi _ \theta (a|s) \over \pi_ {\tilde \theta} (a | s)} &#92;)</span>.

Therefore, let's design a simpler objective that directly constrains <span>&#92;({\pi _ \theta (a|s) \over \pi_ {\tilde \theta} (a | s)} &#92;)</span>. We are going to clip the ratio.

We can use this simpler objective to define a new alg called Proximal Policy Optimization (PPO).

See details in [slides](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-module1.pdf).

# Maximum Entropy Reinforcement Learning
First, let's compare deterministic policies and stochastic policies.

Deterministic
- There always exists an
optimal deterministic policy
- Search space is smaller for
deterministic than
stochastic policies
- Practitioners prefer
deterministic policies

Stochastic
- Search space is continuous
for stochastic policies (helps
with gradient descent)
- More robust (less likely to
overfit)
- Naturally incorporate
exploration
- Facilitate transfer learning
- Mitigate local optima

Based on standard MDP, we add entropy term to encourage stochasticity. Let reward:
<span>&#92;(R(s,a) + \lambda H( \pi (\cdot | s)) &#92;)</span>

Entropy: measure of uncertainty.
<span>&#92;[
    H(p) = -\sum_ x p(x) \log p (x)
&#92;]</span>

Accordingly, we change our policy function and value function to get several soft versions.

Soft Q-learning:
- Q-learning based on Soft Q-Value Iteration
- Replace expectations by samples
- Represent Q-function by a function approximator
(e.g., neural network)
- Do gradient updates based on temporal differences

In practice, actor critic techniques tend to perform
better than Q-learning. Can we derive a soft actor-critic algorithm?

Yes. Critic: soft Q-function. Actor: (greedy) softmax policy.

Soft Actor-Critic
- RL version of soft policy iteration
- Use neural networks to represent policy and value
function
- At each policy improvement step, project new policy
in the space of parameterized neural nets

See the pseudocodes and math derivation in [slides](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-module2.pdf).

# Multi-armed bandits
It is really is RL with single state.
## Exploration/Exploitation Tradeoff
Fundamental problem of RL due to the active
nature of the learning process. When you consider ML in general, like supervised/unsupervised learning, there is no exploration: just feed the data to predictor. In RL, you have to be active to select an action when at some state, and has influence in the future. Now consider one-state RL problems known as
**bandits**.

<div class="fancy-block" data-type="Definition" data-title="Stochastic Bandits">
<div class="fancy-block-content">
    Single state: <span>&#92;(S=&#92;left&#92;{ s &#92;right&#92;} &#92;)</span>. <span>&#92;(A &#92;)</span>: set of action (also known as arms). Space of rewards (often re-scaled to be <span>&#92;([0,1] &#92;)</span>)
</div></div>

No transition function to be learned since there is a single state. We simply need to learn the **stochastic** reward function.

The term bandit comes from gambling where slot
machines can be thought as one-armed bandits.

Problem: which slot
machine should we play at
each turn when their payoffs
are not necessarily the
same and initially unknown?

In practice, there are some examples that can be formed as bandit:
- Design of experiments (Clinical Trials) in health or biology
- Online ad placement
- Web page personalization
- Games
- Networks (packet routing)

## Online Ad Optimization
**Problem**: which ad should be presented?

**Answer**: present ad with highest payoff

<p align="center">payoff = clickThroughRate × payment</p>

- Click through rate: probability that user clicks on ad
- Payment: $$ paid by advertiser
    - Amount determined by an auction

Introduced by Google yrs ago.

Now let's simplify the problem:
- Assume payment is 1 unit for all ads
- Need to estimate click through rate

Formulate as a bandit problem:
- Arms: the set of possible ads
- Rewards: 0 (no click) or 1 (click)

In what order should ads be presented to
maximize revenue? How should we balance exploitation and exploration?

Simple yet difficult problem
- Simple: description of the problem is short
- Difficult: no known tractable optimal solution

Simple heuristics
- Greedy strategy: select the arm with the highest average so far.
    - May get stuck due to lack of exploration
- <span>&#92;(\epsilon &#92;)</span>-greedy: select an arm at random with
probability <span>&#92;(\epsilon &#92;)</span> and otherwise do a greedy selection
    - Convergence rate depends on choice of <span>&#92;(\epsilon &#92;)</span>

## Regret
Let <span>&#92;(R(a) &#92;)</span> be the unknown average reward of <span>&#92;(a &#92;)</span>.

Let <span>&#92;(r^* = \max_ a R(a) &#92;)</span> and <span>&#92;(a^* =\operatorname{argmax} _ aR(a) &#92;)</span>

Denote by <span>&#92;(loss(a) &#92;)</span> be the **expected regret** of <span>&#92;(a &#92;)</span>, for each action
<span>&#92;[
    loss(a)= r^* - R(a)
&#92;]</span>

Denote <span>&#92;(Loss_ n &#92;)</span> the **expected cumulative regret** for <span>&#92;(n &#92;)</span> time steps
<span>&#92;[
    Loss_ n = \sum _ {t=1}^n loss(a_ t)
&#92;]</span>

When <span>&#92;(\epsilon &#92;)</span> is constant, then
- For large enough <span>&#92;(t &#92;)</span>: <span>&#92;(Pr(a _ t \ne a^* ) \approx \epsilon &#92;)</span>
- Expected cumulative regret: <span>&#92;(Loss _ n \approx \sum_ {t=1}^n \epsilon = O(n) &#92;)</span>. Linear regret

When <span>&#92;(\epsilon_ t\propto 1/t &#92;)</span>. Choose it to decrease.
-  For large enough <span>&#92;(t &#92;)</span>: <span>&#92;( Pr(a _ t \ne a^* ) \approx \epsilon_ t = O(1/t )&#92;)</span>
- Expected cumulative regret: <span>&#92;(Loss _ n = \sum _ {t=1}^n {1\over t}= O(\log n) &#92;)</span>. Logarithmic regret.

## Empirical mean

Problem: how far is the empirical mean <span>&#92;(\tilde R(a) &#92;)</span>
from the true mean <span>&#92;(R(a) &#92;)</span>?

If we knew that <span>&#92;(|R(a)-\tilde R(a)|\le bound &#92;)</span>
- Then we wound know that <span>&#92;(R(a)< \tilde R(a)+bound &#92;)</span>
- And we could select the arm with best <span>&#92;(\tilde R(a)+ bound &#92;)</span>

Overtime, additional data will allow us to refine <span>&#92;(\tilde R(a) &#92;)</span> and compute a tighter *bound*

Suppose that we have an oracle that returns an
upper bound <span>&#92;(UB_ n(a) &#92;)</span> on <span>&#92;(R(a) &#92;)</span> for each arm based
on <span>&#92;(n &#92;)</span> trials of arm <span>&#92;(a &#92;)</span>.

Suppose the upper bound returned by this oracle
converges to <span>&#92;(R(a) &#92;)</span> in the limit. Then we can develop **optimistic algorithm**: At each step, select <span>&#92;(\operatorname{argmax}_ a UB_ n (a) &#92;)</span>

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    An optimistic strategy that always selects <span>&#92;(\operatorname{argmax}_ a UB_ n (a) &#92;)</span> will converge to <span>&#92;(a^* &#92;)</span>
</div></div>

Proof by contradiction.

**Problem**: We can't compute an upper bound with certainty since we are sampling. However, we can obtain measures <span>&#92;(f &#92;)</span> that are upper bounds most of the time
- i.e., <span>&#92;(Pr(R(a)\le f(a)) \ge 1-\delta &#92;)</span>
- Example: Hoeffding's ineq:
<span>&#92;[
    \operatorname{Pr}\left(R(a) \leq \tilde{R}(a)+\sqrt{\frac{\log \left(\frac{1}{\delta}\right)}{2 n_ {a}}}\right) \geq 1-\delta
&#92;]</span>
where <span>&#92;(n _ a &#92;)</span> is the number of trials for arm <span>&#92;(a &#92;)</span>

## Upper Confidence Bound (UCB)
Set <span>&#92;(\delta_n = {1\over n^4} &#92;)</span> in Hoeffding's bound. CHoose <span>&#92;(a &#92;)</span> with highest Hoeffding bound

![there should be a image...](/pics/ucb.png)

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    Although Hoeffding's bound is probabilistic, UCB converges.
</div></div>

**Idea**: As <span>&#92;(n &#92;)</span> increases, the term <span>&#92;(\sqrt{2\log n \over n _ a} &#92;)</span> increase, ensuring that all arms are tried infinitely often

Expected cumulative regret is logarithmic regret.

## Summary
- Stochastic bandits
    - Exploration/exploitation tradeoff
- <span>&#92;(\epsilon &#92;)</span>-greedy and UCB
    - Theory: logarithmic expected cumulative regret
- In practice:
    - UCB often performs better than <span>&#92;(\epsilon &#92;)</span>-greedy
    - Many variants of UCB improve performance

# Bayesian
Previously, we examined <span>&#92;(\epsilon &#92;)</span>-greedy and UCB. There are alternative Bayesian approaches: **Thompson sampling** and Gittins indices.

## Bayesian Learning
Notation:
- <span>&#92;(r^a &#92;)</span>: random variable for <span>&#92;(a &#92;)</span>'s rewards
- <span>&#92;(Pr(r^a;\theta) &#92;)</span>: unknown distribution (parametrized by <span>&#92;(\theta &#92;)</span>)
- <span>&#92;(R(a)= E[r^a] &#92;)</span>: unknown avg reward

Idea:
- Express uncertainty about <span>&#92;(\theta &#92;)</span> by a prior <span>&#92;(Pr(\theta) &#92;)</span>
- Compute posterior <span>&#92;(Pr(\theta| r_ 1^a, \ldots , r _ n^a) &#92;)</span> based on samples <span>&#92;(r_ 1 ^a,\ldots, r_ n^a &#92;)</span> observed for <span>&#92;(a &#92;)</span> so far.

By Bayes theorem:
<span>&#92;[
    Pr(\theta| r_ 1^a, \ldots , r _ n^a) \propto Pr(\theta) Pr (r_ 1^a,\ldots, r_ n^ a| \theta )
&#92;]</span>

Posterior over <span>&#92;(\theta &#92;)</span> allows us to estimate
- Distribution over next reward <span>&#92;(r^a &#92;)</span>
<span>&#92;[
    \operatorname{Pr}\left(r^{a} | r&#95;{1}^{a}, r&#95;{2}^{a}, \ldots, r&#95;{n}^{a}\right)=\int&#95;{\theta} \operatorname{Pr}\left(r^{a} ; \theta\right) \operatorname{Pr}\left(\theta | r&#95;{1}^{a}, r&#95;{2}^{a}, \ldots, r&#95;{n}^{a}\right) d \theta
&#92;]</span>
- Distribution over <span>&#92;(R(a) &#92;)</span> when <span>&#92;(\theta &#92;)</span> includes the mean
<span>&#92;[
    Pr(R(a)| r_ 1^a, \ldots , r _ n^a) = Pr(\theta| r_ 1^a, \ldots , r _ n^a) \text{ if }\theta = R(a)
&#92;]</span>

To guide exploration:
- UCB: <span>&#92;(Pr(R(a)\le bound (r_ 1^a, \ldots, r _ n^a))\ge 1-\delta  &#92;)</span>
- Bayesian techniques: <span>&#92;(Pr(R(a)| r_ 1^a, \ldots , r _ n^a) &#92;)</span>

Now consider coin example. Consider two biased coin <span>&#92;(C_ 1 &#92;)</span> and <span>&#92;(C _ 2 &#92;)</span>We want to maximize # of heads in <span>&#92;(k &#92;)</span> flips. Which coin should we choose for each flip?

<span>&#92;(r^{C _ 1}, r^{C _ 2} &#92;)</span> are Bernoulli vars with domain <span>&#92;(&#92;left&#92;{ 0,1 &#92;right&#92;} &#92;)</span>. Bernoulli distribution are parametrized by their mean.

Here we let the prior <span>&#92;(Pr(\theta) &#92;)</span> be a Beta distribution:
<span>&#92;[
    Beta(\theta,\alpha, \beta)\propto \theta^{\alpha-1}(1-\theta)^{\beta-1}
&#92;]</span>

Here <span>&#92;(\alpha-1 &#92;)</span> is # of heads, <span>&#92;(\beta-1 &#92;)</span> is # of tails. Then <span>&#92;(E(\theta)=\alpha/(\alpha+\beta) &#92;)</span>.

Prior: <span>&#92;(Pr(\theta) = Beta(\theta; \alpha,\beta)\propto \theta^{\alpha-1}(1-\theta)^{\beta-1} &#92;)</span>

Posterior after coin flip:
- <span>&#92;(Pr(\theta|head) \propto Beta(\theta;\alpha+1,\beta) &#92;)</span>
- <span>&#92;(Pr(\theta|head) \propto Beta(\theta;\alpha,\beta+1) &#92;)</span>

## Thompson Sampling
Idea:
- Sample several potential average rewards for each <span>&#92;(a &#92;)</span>
- Estimate empirical average
- Execute <span>&#92;(\operatorname{argmax}_a \hat R(a) &#92;)</span>

Thompson  Sampling Algorithm
Bernoulli Rewards

![there should be a image...](/pics/thompson.png)

## Comparison

![there should be a image...](/pics/885compare.png)

Note that on the left, <span>&#92;(i &#92;)</span> means the index of samples that I get from posterior distribution; on the right, it means time step.

In Thompson sampling, amount of data <span>&#92;(n &#92;)</span> and
sample size <span>&#92;(k &#92;)</span> regulate amount of exploration

As <span>&#92;(n &#92;)</span> and <span>&#92;(k &#92;)</span> increases, <span>&#92;(\hat R(a) &#92;)</span> becomes less stochastic, which reduces exploration
- As <span>&#92;(n &#92;)</span> increases, <span>&#92;(Pr( R(a) | r_ 1^a,\ldots, r_ n^a) &#92;)</span> becomes more peaked
- As <span>&#92;(k &#92;)</span> increases, <span>&#92;(\hat R(a) &#92;)</span> approaches <span>&#92;(E[R(a)| r_ 1^a, \ldots, r _ n^a] &#92;)</span>

The stochasticity of <span>&#92;(\hat R(a) &#92;)</span> ensures that all actions are chosen with some probability.

Thus we can show that Thompson sampling converges to best arm. In theory, expected cumulative regret: <span>&#92;(O(\log n) &#92;)</span>, which is on par with UCB and <span>&#92;(\epsilon &#92;)</span>-greedy. In practice: we choose sample size <span>&#92;(k &#92;)</span> often to be 1. It is small because
- computationally faster
- ensure that there is more exploration

# Contextual Bandits
In many applications, the context provides
additional information to select an action. Select action based on contexts.
- E.g., personalized advertising, user interfaces
- Context: user demographics (location, age, gender)

Actions can also be characterized by features
that influence their payoff
- E.g., ads, webpages
- Action features: topics, keywords, etc

**Contextual bandits**: multi-armed bandits with states
(corresponding to contexts) and action features

Formally:
- <span>&#92;(S &#92;)</span>: set of states where each state <span>&#92;(s &#92;)</span> is defined by a vector of features <span>&#92;({\bf x}^s = (x_ 1^s,\ldots, x_ k^s)  &#92;)</span>
- <span>&#92;(A &#92;)</span>: set of actions where each action <span>&#92;(a &#92;)</span> is associated with a vector of features <span>&#92;({\bf x}^a = (x_ 1^a, \ldots, x_ \ell ^a) &#92;)</span>
- Space of rewards (often <span>&#92;(\mathbb R &#92;)</span>)

Still, no transition function since the states at each step are independent. No correlations between each state.

**Goal**: find policy <span>&#92;(\pi:{\bf x}^S\to a &#92;)</span> that maximizes expected rewards <span>&#92;(E(r|s,a)=E(r|{\bf x}^S, {\bf x}^a) &#92;)</span>

A common approach is to learn approximate average reward function <span>&#92;(\tilde R(s,a)=\tilde R({\bf x})=\tilde R({\bf x}^s,{\bf x}^a)  &#92;)</span> by regression. To approximate it, we have linear/non-linear approximation as before.

The rest of math modelling can be found in [lecture 8b](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-lecture8b.pdf):
- Bayesian Linear Regression: find best set of weights that explain how to obtain the rewards by a linear combo of the input features
- Predictive Posterior: With Gaussian distribution, we impose a bound with certain probability. The derivation would require knowledge from probability and statistics.


Then we put these into an algorithm: Upper Confidence Bound (UCB)
Linear Gaussian.

![there should be a image...](/pics/ucb-guassion.png)


Thompson Sampling Algorithm
Linear Gaussian

![there should be a image...](/pics/thompson-gaussian.png)

## Industrial Use
Contextual bandits are now commonly used for
- Personalized advertising
-  Personalized web content
    - [MSN news](https://www.microsoft.com/en-us/research/blog/real-world-interactive-learning-cusp-enabling-new-class-applications/): 26% improvement in click through rate after adoption of contextual bandits

# Model-based RL
Learn explicit transition and/or reward model
- plan based on the model
- **benefit**: increased sample efficiency
- **drawback**: increased complexity

![there should be a image...](/pics/model-based.png)

Idea: at each step
- Execute action
- Observe resulting state and reward
- Update transition and/or reward model
- Update policy and/or value function

## Model-based RL (with Value Iteration)

![there should be a image...](/pics/mb-rl.png)

## Complex models
Use function approx. for transition and reward models
- Linear model
- Non-linear models:
    - Stochastic (e.g., Gaussian process)
    - Deterministic (e.g., neural network)

In complex models, fully optimizing the policy or
value function at each time step is intractable. Thus consider partial planning
- A few steps of Q-learning
- A few steps of policy gradient

## Model-based RL (with Q-learning)
![there should be a image...](/pics/mbrl-q.png)

## Partial Planning vs Replay Buffer
Previous algorithm is very similar to Model-free Q-learning with a replay buffer. Instead of updating Q-function based on samples from
replay buffer, generate samples from model

- Replay buffer:
    -  Simple, real samples, no generalization to other sate-action pairs
- Partial planning with a model
    - Complex, simulated samples, generalization to other state-action pairs (can help or hurt)

## Dyna
Learn explicit transition and/or reward model. Plan based on the model

Learn directly from real experience

![there should be a image...](/pics/dyna.png)

![there should be a image...](/pics/dyna-q.png)

Instead of planning at arbitrary states, plan from
the current state. This helps improve next action. **Monte Carlo Tree Search**.

The idea: we can build a search tree. It will grow exponentially.

![there should be a image...](/pics/tree-search.png)

The question: if we do planning from current state, how tractablly? not to build the entire search tree. Three ideas:
- Leaf nodes: approximate leaf values with value of default policy <span>&#92;(\pi &#92;)</span>. Give us an estimate and cut off the search.
- Chance nodes: approximate expectation by sampling from transition model. Take the sample of the children.
- Decision nodes: expand only most promising actions

If we combine these three ideas, the resulting algorithm: Monte Carlo Tree Search

## Monte Carlo Tree Search
(with upper confidence bound)

![there should be a image...](/pics/uct.png)

<span>&#92;(node_ \ell &#92;)</span> indicates the leaf.


See the helper functions in [lecture 9](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-lecture9.pdf).

Note that in two player mode, one try to minimize, the other one try to maximize the score.

In AlphaGo, four steps:
1. Supervised Learning of Policy Networks
2. Policy gradient with Policy Networks
3. Value gradient with Value Networks
4. Searching with Policy and Value Networks
    - *Monte Carlo Tree Search variant*

# Bayesian RL
This topic is really exciting. None of the textbooks cover it. Reading:  Michael O’Gordon Duff’s PhD Thesis (2002). For this lecture, we will only talk about model-based Bayesian RL.

So far, we have seen model-free and model-based RL.
- Model-free RL: unbiased direct learning, needs many
interactions with environment
- Model-based: biased indirect learning via a model,
if bias is not too important then less data needed

With a model-based approach, the issue is biased model. Problem:
- Model learned from finite amount of data
- Model is necessarily imperfect
- There is a risk that planning will overfit the model inaccuracies and produce a bad policy

<span style="color:green">Solution:  represent uncertainty in model</span>

We are going to model uncertainty, where we have a distribution over possible models.

**Bayesian RL**
- Explicit representation of uncertainty
- Benefits
    - Balance exploration/exploitation tradeoff
    - Mitigate model bias
    - Reduce data needs
- Drawback
    - Complex computation

In Bayesian RL, idea: augment state with distribution about unknown parameters
- Information states: <span>&#92;((\boldsymbol s, \boldsymbol b \in \boldsymbol S\times B) &#92;)</span>
    - Physical states: <span>&#92;(\boldsymbol s\in \boldsymbol S &#92;)</span>
    - Belief states: <span>&#92;(\boldsymbol b\in \boldsymbol B &#92;)</span> where <span>&#92;(b(\theta) = Pr(\theta) &#92;)</span>. If certain, distribution will be quite peak. Otherwise, quite wide.
- Actions: <span>&#92;(\boldsymbol a\in \boldsymbol A &#92;)</span>
- Rewards: <span>&#92;(\boldsymbol r\in\mathbb R &#92;)</span>
- Known model: <span>&#92;(Pr(\boldsymbol r,\boldsymbol s',\boldsymbol b'| \boldsymbol s, \boldsymbol b, \boldsymbol a) &#92;)</span>

Goal: find policy <span>&#92;(\pi:S\times B\to A &#92;)</span> and/or value function <span>&#92;(Q:S\times B\times A\to \mathbb R &#92;)</span>


Now let's look more closely at the model since we **claim** the model in Bayesian RL is know!
<span>&#92;[
\operatorname{Pr}\left(r, s^{\prime}, b^{\prime} | s, b, a\right)=
\underbrace{\operatorname{Pr}\left(r, s^{\prime} | s, b, a\right) } _ {\text{Physical model}}
\underbrace{\operatorname{Pr}\left(b^{\prime} | r, s^{\prime}, s, b, a\right)} _ {\text{belief model}}
&#92;]</span>

Idea:
- For physical, integrate out unknown <span>&#92;(\theta &#92;)</span>
- For belief, <span>&#92;(b' &#92;)</span> is the posterior belief

## Belief state
Let's model our uncertainty with respect to <span>&#92;(\theta &#92;)</span> by a Beta distribution
<span>&#92;[
b(\theta) =k\theta^{\alpha-1}(1-\theta)^{\beta-1}
&#92;]</span>
Belief Update: Bayes theorem
<span>&#92;[
\begin{aligned}
b^{\prime}(\theta) &=b^{s, a, s^{\prime}}(\theta) &#92;&#92;
&=b\left(\theta | s, a, s^{\prime}\right) &#92;&#92;
& \propto b(\theta) \operatorname{Pr}\left(s^{\prime} | s, a, \theta\right)
\end{aligned}
&#92;]</span>

## Physical Model
Consider <span>&#92;(s=(i,j) &#92;)</span>, <span>&#92;(a=right &#92;)</span>, <span>&#92;(s'=(i',j') &#92;)</span> where <span>&#92;(i'=i &#92;)</span> and <span>&#92;(j' = j-1 &#92;)</span>.

Predictive distribution
<span>&#92;[
\begin{aligned}
\operatorname{Pr}\left(s^{\prime} | s, b, a\right) &=\int _ {\theta} \operatorname{Pr}\left(s^{\prime} | s, a, \theta\right) b(\theta) d \theta &#92;&#92;
&=\int _ {\theta} \operatorname{Pr}\left(i^{\prime}, j^{\prime} | i, j, right, \theta\right) \operatorname{Beta}(\theta ; \alpha, \beta) d \theta &#92;&#92;
&=\int _ {\theta} \frac{(1-\theta)}{2} k \theta^{\alpha-1}(1-\theta)^{\beta-1} d \theta &#92;&#92;
&= {\beta \over 2(\alpha + \beta)}
\end{aligned}
&#92;]</span>

## Planning
Since the model is known, treat Bayesian RL as an MDP.

Benefits:
- Solve RL problem by planning (e.g., value/policy iteration)
- Optimal exploration/exploitation tradeoff. We can see  this via Bellman's Equation (augment <span>&#92;(s &#92;)</span> with <span>&#92;(b &#92;) for each occurrence</span>)

Drawback: Complex computation

![there should be a image...](/pics/value-iter-bayes.png)

Then we have only one single objective: max expected total rewards
- <span>&#92;(V^\pi(s,b)=\sum _ t \gamma^t E[r _ t s _ t, b _ t] &#92;)</span>.
- Optimal policy <span>&#92;(\pi^* &#92;)</span>
    - Optimal exploration/exploitation tradeoff (given prior knowledge)

Two phases:
![there should be a image...](/pics/bayes.png)

## Challenges
Offline planning is notoriously difficult
- Use function approximators (e.g., Gaussian process or
neural net) for model, !∗ and #∗
- Continuous belief space
- Problem: a good plan should implicitly account for all
possible environments, which is intractable

Alternative: online partial planning
- Thompson sampling
- PILCO (Model-based Bayesian Actor Critic)

### Thompson Sampling in Bayesian RL
Idea: Sample models <span>&#92;(\theta_i &#92;)</span> at each step and plan for the
corresponding <span>&#92;(MDP_ {\theta_ i} &#92;)</span>'s

![there should be a image...](/pics/ThompsonSamplingInBayesianRL.png)

### Model-based Bayesian Actor Critic
Second approach

PILCO: Deisenroth, Rasmussen (2011). Deep PILCO: Gal, McCallister, Rasmussen (2016), replace Gaussian process transition model (<span>&#92;(b\theta &#92;)</span>) by Bayesian neural network transition model

![there should be a image...](/pics/pilco.png)

Unprecedented Data Efficiency

# Hidden Markov Models
This is the context of partial observability.

![there should be a image...](/pics/observe.png)

Recall our problem, why can we assume the environment reveals the state completely? Instead, the environment provides observation: some info, but might be complete. This is a more general setting.

Goal: Learn to choose actions that maximize rewards

Recall Markov Process
- first-order Markovian
- Stationary

Hidden Markov Model: instead of having process with states, we are going to create observations correlated with states.
Assumptions:
- first-order Markovian
- Stationary
<span>&#92;[
    &#92;begin{aligned}
    &Pr(s_ t | s_ {t-1}) = Pr(s_ {t+1} | s_ t) \quad \forall t &#92;&#92;
    &Pr(o_ t| s_ t) = Pr(o_ {t+1}|s_ {t+1})\quad \forall t
    &#92;end{aligned}
&#92;]</span>

![there should be a image...](/pics/hmm.png)

**Examples**: speech recognition, mobile robot localisation.

Mobile Robot Localisation
- <span>&#92;(s &#92;)</span>: coordinates of  the robot on a map
- <span>&#92;(o &#92;)</span>: distances to surrounding obstacles (measured by laser
range finders or sonars)
- <span>&#92;(Pr(s_ t| s_ {t-1}) &#92;)</span>: movement of the robot with uncertainty
- <span>&#92;(Pr(o_ t| s_ t) &#92;)</span>: uncertainty in the measurements provided by
laser range finders and sonars

Localisation: <span>&#92;(Pr(s_ t| o_ t,\ldots, o_ 1) &#92;)</span>?

More generally, there are several tasks:
- Monitoring: <span>&#92;(Pr(s_ t | o_{ 1\ldots t}) &#92;)</span>
- Prediction: <span>&#92;(Pr(s_ {t+k} | o_ {1\ldots t}) &#92;)</span>
- Hindsight: <span>&#92;(Pr(s_ k| o _ {1\ldots t}) &#92;)</span> where <span>&#92;(k<t &#92;)</span>: estimate some hidden state from the past given some observations. Understanding of a situation or event only after it has happened or developed.
- Most likely explanation: <span>&#92;(\operatorname{argmax} _ {s_ 1,\ldots, s _ t} Pr(s_ {1\ldots t| o _ {1\ldots t}}) &#92;)</span>

What algorithms should we use?

## Monitoring
<span>&#92;(Pr(s_ t| o_ {1..t }) &#92;)</span>:  distribution over current state given observations

Examples: robot localisation, patient monitoring

Recursive computation: <span>&#92;[
    Pr(s _ t | o _ {1.. t}) \propto \ldots = Pr (o _ t| s _ t) \sum _ {s_ {t-1}} Pr(s_ t| s_ {t-1}) Pr (s_ {t-1} |o_ {1..t-1})
&#92;]</span>

### Forward algorithm
Compute <span>&#92;(Pr(s_ t| o_ {1..t}) &#92;)</span> by forward computation. Start with first hidden state. Then apply recursive update. Linear complexity in <span>&#92;(t &#92;)</span>.

## Prediction

We can also generalize this to do a prediction.

<span>&#92;(Pr(s_ {t+k}| o_ {1..t }) &#92;)</span>:   distribution over future state given
observations

Examples: weather prediction, stock market prediction.

Recursive computation: <span>&#92;[
    Pr(s _ {t+k} | o _ {1.. t}) = \ldots = \sum_ {s _ {t+k-1}} Pr (s_ {t+k}| s _ {t+k-1}) Pr (s_ {t+k-1} | o_ {1..t})
&#92;]</span>

Thus this leads to forward algorithm, linear complexity <span>&#92;(t+k &#92;)</span>.

# Partially observable RL
Partially Observable
Markov Decision Process (POMDP)

![there should be a image...](/pics/pomdp.png)

Definition:
- <span style="opacity:0.5">States: <span>&#92;(s\in S &#92;)</span></span>
- <span style="color:red">Observations: <span>&#92;(o\in O &#92;)</span></span>
- Actions: <span>&#92;(a\in A &#92;)</span>
- Rewards: <span>&#92;(r\in \mathbb R &#92;)</span>
- <span style="opacity:0.5">Transition model</span> unknown model
- <span style="opacity:0.5; color:red">Observation model</span> unknown model
- <span style="opacity:0.5">Reward model</span> unknown model
- Discount factor
- Horizon (# of time steps)

Goal: find optimal policy <span>&#92;(\pi^* &#92;)</span> such that
<span>&#92;[
    \pi^* = \operatorname{argmax}_ \pi \sum_ {t=0}^h \gamma^t E_ \pi [r_ t]
&#92;]</span>

Simple Heuristic: Approximate <span>&#92;(s_ y &#92;)</span> by <span>&#92;(o_ t &#92;)</span> or finite window of previous observations.

## Model-based Partially Observable RL
Model-based RL
- Learn HMM from data
- Plan by  optimizing POMDP policy
    - Value iteration, Monte Carlo tree search

![there should be a image...](/pics/model-based-partial.png)

## HMM Parameters
Let <span>&#92;(s_ t\in &#92;left&#92;{ c_ 1, c_ 2 &#92;right&#92;} &#92;)</span> and <span>&#92;(o_ t \in &#92;left&#92;{ v_ 1, v_ 2 &#92;right&#92;} &#92;)</span> (simple states and observation values)

Parameters
- Initial state distribution <span>&#92;(\psi &#92;)</span>
- Transition probabilities <span>&#92;(\theta &#92;)</span>
- Observation probabilities <span>&#92;(\phi &#92;)</span>

One simple to learn HMM from data is to apply maximum likelihood principle. It is supervised learning: <span>&#92;(o &#92;)</span>'s are known.

Objective: <span>&#92;(\operatorname{argmax}_ {\psi,\theta, \phi} Pr(o_ {1..t}, s_ {1..t}| \psi, \theta, \phi) &#92;)</span>
- Set derivative to 0
- Isolate parameters <span>&#92;(\psi, \theta,\phi &#92;)</span>

We record some data (multinomial observations) (see [slides](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-lecture11b.pdf) for details ). Maximum likelihood solution: relative frequency counts.

Or use Gaussian distribution, estimate mean and variance.   

## Planning
Idea:  summarize previous observations into a
distribution about the current unobserved state
called **belief**

Belief: <span>&#92;(b_ t(s_ t) = Pr(s_ t| o_ {1..t}) &#92;)</span>

Then we have recursive formula to do belief monitoring.

Now we can reformulate POMDP as Belief MDP.
- Replace <span>&#92;(s_ t &#92;)</span> by <span>&#92;(b(s _ t) &#92;)</span>
- Action depends only on previous belief

![there should be a image...](/pics/belief-mdp.png)

Then the algorithm

![there should be a image...](/pics/beliefmdp-alg.png)

# Deep Recurrent Q-Networks
Recall HMM and belief monitoring. It turns out we don't have to use HMM to compute beliefs. We can use **Recurrent Neural Network** (RNN) to do the same.
- In RNNs, outputs can be fed back to the network as
inputs, creating a recurrent structure
- HMMs can be simulated and generalized by RNNs
- RNNs can be used for belief monitoring

<span>&#92;(x_ t &#92;)</span>: vector of observations. <span>&#92;(h_ t &#92;)</span>: belief state.

![there should be a image...](/pics/rnn.png)

<span>&#92;(f &#92;)</span> in principle, can be neural networks of several layers which is quite general. We can also set <span>&#92;(f &#92;)</span> to be the equation there (belief monitoring recursive updating). The beauty is we are not restricted to that, we can define whatever we want.

## Training
- Recurrent neural networks are trained by
backpropagation on the unrolled network
    - backpropagation through time
- Weight sharing:
    - Combine gradients of shared weights into a single gradient
- Challenges:
    - Gradient vanishing (and explosion). When you unroll your network, you are creating DNN. The depth will amplify gradient vanishing.
    - Long range memory
    - Prediction drift

## Long Short Term Memory (LSTM)
![there should be a image...](/pics/ltsm.png)

- Special gated structure to
control memorization and
forgetting in RNNs
- Mitigate gradient vanishing
- Facilitate long term memory

Unrolled long short term memory

![there should be a image...](/pics/unrolled-lstm.png)

The gate will determine what goes out.

See the algorithms on [the last two slides](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-lecture12.pdf).

# Imitation Learning
- Behavioural cloning (supervised learning)
- Generative adversarial imitation learning (GAIL)
- Imitation learning from observations
- Inverse reinforcement learning (In the next module).

Motivation: learn from expert demonstrations. Thus no reward function needed, faster learning.

**Behavioural cloning**:

Simplest form of imitation learning. Assumption: state-action pairs observable

We observe trajectories: <span>&#92;((s_ 1,a _ 1), \ldots, (s_ n , a _ n) &#92;)</span>. Then create training set: <span>&#92;(S\to A &#92;)</span>. Train by supervised learning.

**Generative adversarial imitation learning (GAIL)**

Common approach: training generator to maximize likelihood of expert actions

Alternative: train generator to fool a discriminator in
believing that the generated actions are from expert. Leverage GANs (Generative adversarial networks)

## Imitation Learning from Observations
Consider imitation learning from a human expert: Actions (e.g., forces) unobservable. Only states/observations (e.g., joint positions) observable. Problem: infer actions from state/observation sequences

Two steps:
1. Learn inverse dynamics
    - Learn <span>&#92;(Pr(a|s,s') &#92;)</span> by supervised learning
    - From <span>&#92;(s,a,s' &#92;)</span> samples obtained by executing random actions
2. Behavioural cloning
    - Learn <span>&#92;(\pi (\hat a | s) &#92;)</span> by supervised learning
    - From <span>&#92;((s,s') &#92;)</span> samples from expert trajectories and from <span>&#92;(\hat a \sim  Pr(a | s,s') &#92;)</span> sampled by inverse dynamics

See the pseudocodes in the [slides](https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-module3.pdf)
